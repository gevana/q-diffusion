{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "#sys.path.append('/work/qdiff/mo_utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libtmux not installed ??\n"
     ]
    }
   ],
   "source": [
    "from mo_utils.utils.tmux_utils import get_session_list,tmux_session,get_session_name,kill_session\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w8bit_nosym = '/fastdata/users/nadavg/sd/qdiff/output_quantization/2025-01-22-16-39-34/wc_ckpt.pth'\n",
    "w8bit_sym = \"/fastdata/users/nadavg/sd/qdiff/output_quantization/2025-01-22-16-40-17/wc_ckpt.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Session($3 sd_eval_wb=4_sw=False_qa=False_gpu_5),\n",
       " Session($0 sd_quantize_wb=8_gpu_3),\n",
       " Session($1 sd_quantize_wb=8_gpu_7)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_session_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kill sessions=[Session($0 sd_eval_wb=4_sw=False_qa=False_gpu_5), Session($1 sd_eval_wb=8_sw=False_qa=False_gpu_5)]\n",
      "after kill sessions=[]\n"
     ]
    }
   ],
   "source": [
    "kill_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = \"a puppy wearing a hat\" \n",
    "task = \"eval\"\n",
    "prompt = \"a puppy wearing a hat\" \n",
    "weight_bit = 4\n",
    "outdir= \"output_pupy\"\n",
    "symmetric_weight = False\n",
    "quant_act = False\n",
    "quantized_ckpt_path = '/fastdata/users/nadavg/sd/qdiff/sd_w4a8_ckpt.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = \"a puppy wearing a hat\" \n",
    "task = \"eval\"\n",
    "prompt = \"a puppy wearing a hat\" \n",
    "weight_bit = 8\n",
    "outdir= \"output_pupy\"\n",
    "symmetric_weight = False\n",
    "quant_act = False\n",
    "quantized_ckpt_path = '/fastdata/users/nadavg/sd/qdiff/sd_w8a8_ckpt.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = \"a puppy wearing a hat\" \n",
    "task = \"eval\"\n",
    "prompt = \"a puppy wearing a hat\" \n",
    "weight_bit = 8\n",
    "outdir= \"output_pupy\"\n",
    "quant_act = False\n",
    "\n",
    "#symmetric_weight = False\n",
    "#quantized_ckpt_path = w8bit_nosym\n",
    "\n",
    "symmetric_weight = True\n",
    "quantized_ckpt_path = w8bit_sym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python scripts/txt2img.py --prompt 'a puppy wearing a hat' --plms --cond --ptq --weight_bit 8 --quant_mode qdiff --no_grad_ckpt --split --n_samples 5 --resume --act_bit 8 --sm_abit 16 --outdir output_pupy --cali_ckpt /fastdata/users/nadavg/sd/qdiff/output_quantization/2025-01-22-16-40-17/wc_ckpt.pth --symmetric_weight \n"
     ]
    }
   ],
   "source": [
    "cmd = (f\"python scripts/txt2img.py --prompt '{prompt}' --plms --cond --ptq --weight_bit {weight_bit} --quant_mode qdiff \"+\n",
    "        f\"--no_grad_ckpt --split --n_samples 5 --resume \" + \n",
    "        quant_act*\"--quant_act \"+\n",
    "        f\"--act_bit 8 --sm_abit 16 --outdir {outdir} --cali_ckpt {quantized_ckpt_path} \"+\n",
    "        symmetric_weight*\"--symmetric_weight \"\n",
    "        )\n",
    "\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_list = [f'cd {Path.home() / \"q-diffusion\"}',\n",
    "             f'conda activate qdiff',\n",
    "             f'export CUDA_VISIBLE_DEVICES={gpu}',\n",
    "             cmd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sd_eval_wb=8_sw=True_qa=False_gpu_5'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess_name = get_session_name(f'sd_{task}_wb={weight_bit}_sw={symmetric_weight}_qa={quant_act}_gpu_{gpu}')\n",
    "sess_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmux attach -t \"sd_eval_wb=8_sw=True_qa=False_gpu_5\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sd_eval_wb=8_sw=True_qa=False_gpu_5'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmux_session(sess_name,inst_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kill sessions=[Session($0 sd_eval_wb=8_sw=False_qa=False_gpu_5)]\n",
      "after kill sessions=[]\n"
     ]
    }
   ],
   "source": [
    "kill_session(kill_only=sess_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'quantize'\n",
    "gpu = 6\n",
    "prompt = \"an astronaut riding a unicorn in space\" \n",
    "weight_bit = 8\n",
    "symmetric_weight = False\n",
    "resume_w = False\n",
    "bs = 8\n",
    "outdir= \"output_quantization\"\n",
    "quantized_ckpt_path = '/fastdata/users/nadavg/sd/qdiff/sd_w8a8_ckpt.pth'\n",
    "cali_data_path='/fastdata/users/nadavg/sd/qdiff/sd_coco-s75_sample1024_allst.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'quantize'\n",
    "gpu = 7\n",
    "prompt = \"an astronaut riding a unicorn in space\" \n",
    "weight_bit = 8\n",
    "symmetric_weight = True\n",
    "bs = 8\n",
    "outdir= \"output_quantization\"\n",
    "resume_w = False\n",
    "quantized_ckpt_path = '/home/nadavg/q-diffusion/output_quantization/8bit_sym/wc_ckpt.pth'\n",
    "cali_data_path='/fastdata/users/nadavg/sd/qdiff/sd_coco-s75_sample1024_allst.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'quantize'\n",
    "gpu = 2\n",
    "prompt = \"an astronaut riding a unicorn in space\" \n",
    "weight_bit = 4\n",
    "symmetric_weight = False\n",
    "bs = 8\n",
    "outdir= \"output_quantization\"\n",
    "resume_w = False\n",
    "quantized_ckpt_path = '/home/nadavg/q-diffusion/output_quantization/2025-01-19-18-27-17/wc_ckpt.pth'\n",
    "cali_data_path= '/fastdata/users/nadavg/sd/qdiff/sd_coco-s75_sample1024_allst.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd=(f\"python scripts/txt2img.py --prompt '{prompt}' --plms --cond --ptq --weight_bit {weight_bit} --quant_mode qdiff \"+\n",
    "    f\"--quant_act --act_bit 8 --cali_st 25 --cali_batch_size {bs} --cali_n 128 --no_grad_ckpt --split --running_stat \"+\n",
    "    f\"--sm_abit 16 --cali_data_path {cali_data_path} --outdir {outdir}\"+\n",
    "    symmetric_weight*\" --symmetric_weight \"+\n",
    "    resume_w*f\"--resume_w --cali_ckpt {quantized_ckpt_path}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"python scripts/txt2img.py --prompt 'an astronaut riding a unicorn in space' --plms --cond --ptq --weight_bit 4 --quant_mode qdiff --quant_act --act_bit 8 --cali_st 25 --cali_batch_size 8 --cali_n 128 --no_grad_ckpt --split --running_stat --sm_abit 16 --cali_data_path /fastdata/users/nadavg/sd/qdiff/sd_coco-s75_sample1024_allst.pt --outdir output_quantization\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_list = [f'cd {Path.home() / \"q-diffusion\"}',\n",
    "             f'conda activate qdiff',\n",
    "             f'export CUDA_VISIBLE_DEVICES={gpu}',\n",
    "             cmd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cd /home/nadavg/q-diffusion',\n",
       " 'conda activate qdiff',\n",
       " 'export CUDA_VISIBLE_DEVICES=2',\n",
       " \"python scripts/txt2img.py --prompt 'an astronaut riding a unicorn in space' --plms --cond --ptq --weight_bit 4 --quant_mode qdiff --quant_act --act_bit 8 --cali_st 25 --cali_batch_size 8 --cali_n 128 --no_grad_ckpt --split --running_stat --sm_abit 16 --cali_data_path /fastdata/users/nadavg/sd/qdiff/sd_coco-s75_sample1024_allst.pt --outdir output_quantization\"]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sd_quantize_wb=4_gpu_2'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess_name = get_session_name(f'sd_{task}_wb={weight_bit}_gpu_{gpu}')\n",
    "sess_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmux attach -t \"sd_quantize_wb=4_gpu_2\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sd_quantize_wb=4_gpu_2'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmux_session(sess_name,inst_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kill sessions=[Session($0 0), Session($6 sd_quantize_wb=4_gpu_6), Session($5 sd_quantize_wb=8_gpu_5)]\n",
      "after kill sessions=[Session($0 0), Session($6 sd_quantize_wb=4_gpu_6)]\n"
     ]
    }
   ],
   "source": [
    "kill_session(kill_only=sess_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before kill sessions=[Session($0 0), Session($2 sd_quantize_wb=8_gpu_4)]\n",
      "after kill sessions=[Session($0 0)]\n"
     ]
    }
   ],
   "source": [
    "#sess_name = \"sd_quantize_wb=8_gpu_7\"\n",
    "kill_session(kill_only=sess_name) \n",
    "#(t sess_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/conda/envs/qdiff_reorg/bin/python'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mo_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_638729/2801563443.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmo_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_to_pt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcreate_torch_script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mo_utils'"
     ]
    }
   ],
   "source": [
    "from mo_utils.utils.torch_utils import torch_to_pt,create_torch_script\n",
    "import netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from txt2img import load_model_from_config\n",
    "from ldm.models.diffusion.plms import PLMSSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'logit_scale', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "config = OmegaConf.load('/work/qdiff/q-diffusion-org/configs/stable-diffusion/v1-inference.yaml')\n",
    "model = load_model_from_config(config, \"/fastdata/users/nadavg/sd/qdiff/sd-v1-4.ckpt\")\n",
    "sampler = PLMSSampler(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.shape = torch.Size([3, 4, 64, 64])\n",
    "\n",
    "t.shape = torch.Size([3]) \n",
    "\n",
    "c.shape = torch.Size([3, 77, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1208315/1280224373.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x = torch.load('/home/nadavg/q-diffusion/temp/x.pth')\n"
     ]
    }
   ],
   "source": [
    "x = torch.load('/home/nadavg/q-diffusion/temp/x.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(60571092.), tensor(-49043352.))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.max(),x.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(429860.5938)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = (x.max() - x.min()) / (2 ** 8 - 1) \n",
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(114.)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_point = (- x.min() / delta).round()\n",
    "zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_int = torch.round(x / delta)\n",
    "x_quant = torch.clamp(x_int + zero_point, 0, 256 - 1)\n",
    "x_float_q = (x_quant - zero_point) * delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(60610344.), tensor(-49004108.))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_float_q.max(),x_float_q.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4398e+10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_float_q-x).abs().pow(2).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(x,bins=50,hrange=None,ax=None,normalized = True):\n",
    "    hx,binx=np.histogram(x,bins=bins,range=hrange)\n",
    "    if normalized:\n",
    "        hx = hx/sum(hx)\n",
    "    centers = (binx[:-1]+binx[1:])/2\n",
    "    widths = binx[1:]-binx[:-1]\n",
    "    if ax is None:\n",
    "        f,ax = plt.subplots()\n",
    "    ax.bar(centers,hx,width=widths)\n",
    "    return hx,binx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_=plot_hist(x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_=plot_hist((x-x_float_q).abs().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4398e+10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x-x_float_q).abs().pow(2).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qdiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
