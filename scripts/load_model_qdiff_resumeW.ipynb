{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nadavg/anaconda3/envs/qdiff/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,sys\n",
    "sys.path.append('/work/qdiff/mo_utils')\n",
    "sys.executable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mo_utils.utils.torch_utils import torch_to_pt,create_torch_script\n",
    "import netron\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdiff import (\n",
    "    QuantModel, QuantModule, BaseQuantBlock, \n",
    "    block_reconstruction, layer_reconstruction,\n",
    ")\n",
    "from qdiff.utils import resume_cali_model, get_train_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nadavg/anaconda3/envs/qdiff/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/nadavg/anaconda3/envs/qdiff/lib/python3.8/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from txt2img import load_model_from_config\n",
    "from ldm.models.diffusion.plms import PLMSSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cali_ckpt = '/fastdata/users/nadavg/sd/qdiff/sd_w4a8_ckpt.pth'\n",
    "#cali_ckpt,nbit,symmetric   = '/fastdata/users/nadavg/sd/qdiff/sd_w8a8_ckpt.pth' ,8,False\n",
    "#w8bit_nosym,nbit,symmetric = '/fastdata/users/nadavg/sd/qdiff/output_quantization/2025-01-22-16-39-34/wc_ckpt.pth',8,False\n",
    "#w8bit_sym = \"/fastdata/users/nadavg/sd/qdiff/output_quantization/2025-01-22-16-40-17/wc_ckpt.pth\"\n",
    "#w8bit_sym,nbit,symmetric   = '/home/nadavg/q-diffusion/output_quantization2/2025-01-24-00-49-55/wc_ckpt.pth',8,True\n",
    "#w8bit_sym,nbit,symmetric = '/home/nadavg/q-diffusion/output_quantization3/2025-01-26-18-44-03/wc_ckpt.pth',8,True\n",
    "w8bit_sym,nbit,symmetric = '/fastdata/users/nadavg/sd/qdiff/output_quantization/2025-01-26-18-44-03/ckpt.pth',8,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ckpt_full = torch.load(cali_ckpt, map_location='cpu')\n",
    "#ckpt_weight_only = torch.load(weight_ckpt, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import garbage collector\n",
    "from gc import collect as gc\n",
    "#from torch import g\n",
    "gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nadavg/q-diffusion/scripts/txt2img.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pl_sd = torch.load(ckpt, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'logit_scale', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.embeddings.class_embedding']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "config = OmegaConf.load(f'{Path.home()}/q-diffusion/configs/stable-diffusion/v1-inference.yaml')\n",
    "model = load_model_from_config(config, \"/fastdata/users/nadavg/sd/qdiff/sd-v1-4.ckpt\")\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "sampler = PLMSSampler(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(sampler.model.model.diffusion_model, \"split\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wq_params = {'n_bits': 4, 'channel_wise': True, 'scale_method': 'mse'}\n",
    "wq_params = {'n_bits': nbit, 'channel_wise': True, 'scale_method': 'mse', 'symmetric': symmetric}\n",
    "\n",
    "aq_params = {'n_bits': 8, 'channel_wise': False, 'scale_method': 'mse', 'leaf_param':  True}\n",
    "wq_params['scale_method'] = 'max'\n",
    "aq_params['scale_method'] = 'max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdiff.quant_layer import QuantModule,QuantOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=sampler.model.model.diffusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiLU()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsb = list(df.named_children())[1][1][1][0]#.input_blocks\n",
    "rsb.in_layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "qop=QuantOp(rsb.in_layers[0],aq_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(qop,torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn = QuantModel(model=sampler.model.model.diffusion_model,\n",
    "                weight_quant_params=wq_params, act_quant_params=aq_params,\n",
    "                act_quant_mode=\"qdiff\", sm_abit=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ldm.modules.diffusionmodules.openaimodel.ResBlock: qdiff.quant_block.QuantResBlock,\n",
       " ldm.modules.attention.BasicTransformerBlock: qdiff.quant_block.QuantBasicTransformerBlock,\n",
       " ddim.models.diffusion.ResnetBlock: qdiff.quant_block.QuantResnetBlock,\n",
       " ddim.models.diffusion.AttnBlock: qdiff.quant_block.QuantAttnBlock,\n",
       " ldm.modules.diffusionmodules.openaimodel.QKMatMul: qdiff.quant_block.QuantQKMatMul,\n",
       " ldm.modules.diffusionmodules.openaimodel.SMVMatMul: qdiff.quant_block.QuantSMVMatMul}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnn.specials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn.cuda()\n",
    "qnn.eval()\n",
    "qnn.set_grad_ckpt(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fastdata/users/nadavg/sd/qdiff/output_quantization/2025-01-26-18-44-03/ckpt.pth'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w8bit_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_data = (torch.randn(1, 4, 64, 64), torch.randint(0, 1000, (1,)), torch.randn(1, 77, 768))\n",
    "resume_cali_model(qnn, w8bit_sym, cali_data, True, \"qdiff\", cond=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_data = (torch.randn(1, 4, 64, 64), torch.randint(0, 1000, (1,)), torch.randn(1, 77, 768))\n",
    "qnn.set_quant_state(True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_xs, cali_ts, cali_cs = cali_data\n",
    "_ = qnn(cali_xs[:1].cuda(), cali_ts[:1].cuda(), cali_cs[:1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sampler.model.model.diffusion_model)\n",
    "sampler.model.model.diffusion_model.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add full name to each module\n",
    "for name, module in qnn.named_modules():\n",
    "    module.full_name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: model.output_blocks.0.0.skip_connection\n"
     ]
    }
   ],
   "source": [
    "from qdiff.quant_layer import UniformAffineQuantizer\n",
    "for name, module in qnn.named_modules():\n",
    "    if isinstance(module, QuantModule):\n",
    "        if module.split:\n",
    "            print('split:' ,module.full_name)# = name\n",
    "            break\n",
    "        #else:\n",
    "        #    print('no split:' ,module.full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantModule(\n",
       "  2560, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=True, channel_wise=True, leaf_param=False)\n",
       "  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "  (activation_function): StraightThrough()\n",
       "  (weight_quantizer_0): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=True, channel_wise=True, leaf_param=False)\n",
       "  (act_quantizer_0): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no leaf: model.time_embed.0.weight_quantizer\n",
      "leaf: model.time_embed.0.act_quantizer\n",
      "leaf: model.time_embed.1.act_quantizer\n",
      "no leaf: model.time_embed.2.weight_quantizer\n",
      "leaf: model.time_embed.2.act_quantizer\n",
      "no leaf: model.input_blocks.0.0.weight_quantizer\n",
      "leaf: model.input_blocks.0.0.act_quantizer\n",
      "leaf: model.input_blocks.1.0.act_quantizer\n",
      "leaf: model.input_blocks.1.0.in_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.1.0.in_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.1.0.in_layers.2.weight_quantizer\n",
      "leaf: model.input_blocks.1.0.in_layers.2.act_quantizer\n",
      "leaf: model.input_blocks.1.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.input_blocks.1.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.input_blocks.1.0.emb_layers.1.act_quantizer\n",
      "leaf: model.input_blocks.1.0.out_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.1.0.out_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.1.0.out_layers.3.weight_quantizer\n",
      "leaf: model.input_blocks.1.0.out_layers.3.act_quantizer\n",
      "no leaf: model.input_blocks.1.1.proj_in.weight_quantizer\n",
      "leaf: model.input_blocks.1.1.proj_in.act_quantizer\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.input_blocks.1.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.input_blocks.1.1.proj_out.weight_quantizer\n",
      "leaf: model.input_blocks.1.1.proj_out.act_quantizer\n",
      "leaf: model.input_blocks.2.0.act_quantizer\n",
      "leaf: model.input_blocks.2.0.in_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.2.0.in_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.2.0.in_layers.2.weight_quantizer\n",
      "leaf: model.input_blocks.2.0.in_layers.2.act_quantizer\n",
      "leaf: model.input_blocks.2.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.input_blocks.2.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.input_blocks.2.0.emb_layers.1.act_quantizer\n",
      "leaf: model.input_blocks.2.0.out_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.2.0.out_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.2.0.out_layers.3.weight_quantizer\n",
      "leaf: model.input_blocks.2.0.out_layers.3.act_quantizer\n",
      "no leaf: model.input_blocks.2.1.proj_in.weight_quantizer\n",
      "leaf: model.input_blocks.2.1.proj_in.act_quantizer\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.input_blocks.2.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.input_blocks.2.1.proj_out.weight_quantizer\n",
      "leaf: model.input_blocks.2.1.proj_out.act_quantizer\n",
      "no leaf: model.input_blocks.3.0.op.weight_quantizer\n",
      "leaf: model.input_blocks.3.0.op.act_quantizer\n",
      "leaf: model.input_blocks.4.0.act_quantizer\n",
      "leaf: model.input_blocks.4.0.in_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.4.0.in_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.4.0.in_layers.2.weight_quantizer\n",
      "leaf: model.input_blocks.4.0.in_layers.2.act_quantizer\n",
      "leaf: model.input_blocks.4.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.input_blocks.4.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.input_blocks.4.0.emb_layers.1.act_quantizer\n",
      "leaf: model.input_blocks.4.0.out_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.4.0.out_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.4.0.out_layers.3.weight_quantizer\n",
      "leaf: model.input_blocks.4.0.out_layers.3.act_quantizer\n",
      "no leaf: model.input_blocks.4.0.skip_connection.weight_quantizer\n",
      "leaf: model.input_blocks.4.0.skip_connection.act_quantizer\n",
      "no leaf: model.input_blocks.4.1.proj_in.weight_quantizer\n",
      "leaf: model.input_blocks.4.1.proj_in.act_quantizer\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.input_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.input_blocks.4.1.proj_out.weight_quantizer\n",
      "leaf: model.input_blocks.4.1.proj_out.act_quantizer\n",
      "leaf: model.input_blocks.5.0.act_quantizer\n",
      "leaf: model.input_blocks.5.0.in_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.5.0.in_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.5.0.in_layers.2.weight_quantizer\n",
      "leaf: model.input_blocks.5.0.in_layers.2.act_quantizer\n",
      "leaf: model.input_blocks.5.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.input_blocks.5.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.input_blocks.5.0.emb_layers.1.act_quantizer\n",
      "leaf: model.input_blocks.5.0.out_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.5.0.out_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.5.0.out_layers.3.weight_quantizer\n",
      "leaf: model.input_blocks.5.0.out_layers.3.act_quantizer\n",
      "no leaf: model.input_blocks.5.1.proj_in.weight_quantizer\n",
      "leaf: model.input_blocks.5.1.proj_in.act_quantizer\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.input_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.input_blocks.5.1.proj_out.weight_quantizer\n",
      "leaf: model.input_blocks.5.1.proj_out.act_quantizer\n",
      "no leaf: model.input_blocks.6.0.op.weight_quantizer\n",
      "leaf: model.input_blocks.6.0.op.act_quantizer\n",
      "leaf: model.input_blocks.7.0.act_quantizer\n",
      "leaf: model.input_blocks.7.0.in_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.7.0.in_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.7.0.in_layers.2.weight_quantizer\n",
      "leaf: model.input_blocks.7.0.in_layers.2.act_quantizer\n",
      "leaf: model.input_blocks.7.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.input_blocks.7.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.input_blocks.7.0.emb_layers.1.act_quantizer\n",
      "leaf: model.input_blocks.7.0.out_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.7.0.out_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.7.0.out_layers.3.weight_quantizer\n",
      "leaf: model.input_blocks.7.0.out_layers.3.act_quantizer\n",
      "no leaf: model.input_blocks.7.0.skip_connection.weight_quantizer\n",
      "leaf: model.input_blocks.7.0.skip_connection.act_quantizer\n",
      "no leaf: model.input_blocks.7.1.proj_in.weight_quantizer\n",
      "leaf: model.input_blocks.7.1.proj_in.act_quantizer\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.input_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.input_blocks.7.1.proj_out.weight_quantizer\n",
      "leaf: model.input_blocks.7.1.proj_out.act_quantizer\n",
      "leaf: model.input_blocks.8.0.act_quantizer\n",
      "leaf: model.input_blocks.8.0.in_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.8.0.in_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.8.0.in_layers.2.weight_quantizer\n",
      "leaf: model.input_blocks.8.0.in_layers.2.act_quantizer\n",
      "leaf: model.input_blocks.8.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.input_blocks.8.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.input_blocks.8.0.emb_layers.1.act_quantizer\n",
      "leaf: model.input_blocks.8.0.out_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.8.0.out_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.8.0.out_layers.3.weight_quantizer\n",
      "leaf: model.input_blocks.8.0.out_layers.3.act_quantizer\n",
      "no leaf: model.input_blocks.8.1.proj_in.weight_quantizer\n",
      "leaf: model.input_blocks.8.1.proj_in.act_quantizer\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.input_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.input_blocks.8.1.proj_out.weight_quantizer\n",
      "leaf: model.input_blocks.8.1.proj_out.act_quantizer\n",
      "no leaf: model.input_blocks.9.0.op.weight_quantizer\n",
      "leaf: model.input_blocks.9.0.op.act_quantizer\n",
      "leaf: model.input_blocks.10.0.act_quantizer\n",
      "leaf: model.input_blocks.10.0.in_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.10.0.in_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.10.0.in_layers.2.weight_quantizer\n",
      "leaf: model.input_blocks.10.0.in_layers.2.act_quantizer\n",
      "leaf: model.input_blocks.10.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.input_blocks.10.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.input_blocks.10.0.emb_layers.1.act_quantizer\n",
      "leaf: model.input_blocks.10.0.out_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.10.0.out_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.10.0.out_layers.3.weight_quantizer\n",
      "leaf: model.input_blocks.10.0.out_layers.3.act_quantizer\n",
      "leaf: model.input_blocks.11.0.act_quantizer\n",
      "leaf: model.input_blocks.11.0.in_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.11.0.in_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.11.0.in_layers.2.weight_quantizer\n",
      "leaf: model.input_blocks.11.0.in_layers.2.act_quantizer\n",
      "leaf: model.input_blocks.11.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.input_blocks.11.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.input_blocks.11.0.emb_layers.1.act_quantizer\n",
      "leaf: model.input_blocks.11.0.out_layers.0.act_quantizer\n",
      "leaf: model.input_blocks.11.0.out_layers.1.act_quantizer\n",
      "no leaf: model.input_blocks.11.0.out_layers.3.weight_quantizer\n",
      "leaf: model.input_blocks.11.0.out_layers.3.act_quantizer\n",
      "leaf: model.middle_block.0.act_quantizer\n",
      "leaf: model.middle_block.0.in_layers.0.act_quantizer\n",
      "leaf: model.middle_block.0.in_layers.1.act_quantizer\n",
      "no leaf: model.middle_block.0.in_layers.2.weight_quantizer\n",
      "leaf: model.middle_block.0.in_layers.2.act_quantizer\n",
      "leaf: model.middle_block.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.middle_block.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.middle_block.0.emb_layers.1.act_quantizer\n",
      "leaf: model.middle_block.0.out_layers.0.act_quantizer\n",
      "leaf: model.middle_block.0.out_layers.1.act_quantizer\n",
      "no leaf: model.middle_block.0.out_layers.3.weight_quantizer\n",
      "leaf: model.middle_block.0.out_layers.3.act_quantizer\n",
      "no leaf: model.middle_block.1.proj_in.weight_quantizer\n",
      "leaf: model.middle_block.1.proj_in.act_quantizer\n",
      "leaf: model.middle_block.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.middle_block.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.middle_block.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.middle_block.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.middle_block.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.middle_block.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.middle_block.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.middle_block.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.middle_block.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.middle_block.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.middle_block.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.middle_block.1.proj_out.weight_quantizer\n",
      "leaf: model.middle_block.1.proj_out.act_quantizer\n",
      "leaf: model.middle_block.2.act_quantizer\n",
      "leaf: model.middle_block.2.in_layers.0.act_quantizer\n",
      "leaf: model.middle_block.2.in_layers.1.act_quantizer\n",
      "no leaf: model.middle_block.2.in_layers.2.weight_quantizer\n",
      "leaf: model.middle_block.2.in_layers.2.act_quantizer\n",
      "leaf: model.middle_block.2.emb_layers.0.act_quantizer\n",
      "no leaf: model.middle_block.2.emb_layers.1.weight_quantizer\n",
      "leaf: model.middle_block.2.emb_layers.1.act_quantizer\n",
      "leaf: model.middle_block.2.out_layers.0.act_quantizer\n",
      "leaf: model.middle_block.2.out_layers.1.act_quantizer\n",
      "no leaf: model.middle_block.2.out_layers.3.weight_quantizer\n",
      "leaf: model.middle_block.2.out_layers.3.act_quantizer\n",
      "leaf: model.output_blocks.0.0.act_quantizer\n",
      "leaf: model.output_blocks.0.0.in_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.0.0.in_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.0.0.in_layers.2.weight_quantizer\n",
      "leaf: model.output_blocks.0.0.in_layers.2.act_quantizer\n",
      "leaf: model.output_blocks.0.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.output_blocks.0.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.output_blocks.0.0.emb_layers.1.act_quantizer\n",
      "leaf: model.output_blocks.0.0.out_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.0.0.out_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.0.0.out_layers.3.weight_quantizer\n",
      "leaf: model.output_blocks.0.0.out_layers.3.act_quantizer\n",
      "no leaf: model.output_blocks.0.0.skip_connection.weight_quantizer\n",
      "leaf: model.output_blocks.0.0.skip_connection.act_quantizer\n",
      "leaf: model.output_blocks.1.0.act_quantizer\n",
      "leaf: model.output_blocks.1.0.in_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.1.0.in_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.1.0.in_layers.2.weight_quantizer\n",
      "leaf: model.output_blocks.1.0.in_layers.2.act_quantizer\n",
      "leaf: model.output_blocks.1.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.output_blocks.1.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.output_blocks.1.0.emb_layers.1.act_quantizer\n",
      "leaf: model.output_blocks.1.0.out_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.1.0.out_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.1.0.out_layers.3.weight_quantizer\n",
      "leaf: model.output_blocks.1.0.out_layers.3.act_quantizer\n",
      "no leaf: model.output_blocks.1.0.skip_connection.weight_quantizer\n",
      "leaf: model.output_blocks.1.0.skip_connection.act_quantizer\n",
      "leaf: model.output_blocks.2.0.act_quantizer\n",
      "leaf: model.output_blocks.2.0.in_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.2.0.in_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.2.0.in_layers.2.weight_quantizer\n",
      "leaf: model.output_blocks.2.0.in_layers.2.act_quantizer\n",
      "leaf: model.output_blocks.2.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.output_blocks.2.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.output_blocks.2.0.emb_layers.1.act_quantizer\n",
      "leaf: model.output_blocks.2.0.out_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.2.0.out_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.2.0.out_layers.3.weight_quantizer\n",
      "leaf: model.output_blocks.2.0.out_layers.3.act_quantizer\n",
      "no leaf: model.output_blocks.2.0.skip_connection.weight_quantizer\n",
      "leaf: model.output_blocks.2.0.skip_connection.act_quantizer\n",
      "no leaf: model.output_blocks.2.1.conv.weight_quantizer\n",
      "leaf: model.output_blocks.2.1.conv.act_quantizer\n",
      "leaf: model.output_blocks.3.0.act_quantizer\n",
      "leaf: model.output_blocks.3.0.in_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.3.0.in_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.3.0.in_layers.2.weight_quantizer\n",
      "leaf: model.output_blocks.3.0.in_layers.2.act_quantizer\n",
      "leaf: model.output_blocks.3.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.output_blocks.3.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.output_blocks.3.0.emb_layers.1.act_quantizer\n",
      "leaf: model.output_blocks.3.0.out_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.3.0.out_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.3.0.out_layers.3.weight_quantizer\n",
      "leaf: model.output_blocks.3.0.out_layers.3.act_quantizer\n",
      "no leaf: model.output_blocks.3.0.skip_connection.weight_quantizer\n",
      "leaf: model.output_blocks.3.0.skip_connection.act_quantizer\n",
      "no leaf: model.output_blocks.3.1.proj_in.weight_quantizer\n",
      "leaf: model.output_blocks.3.1.proj_in.act_quantizer\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.output_blocks.3.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.output_blocks.3.1.proj_out.weight_quantizer\n",
      "leaf: model.output_blocks.3.1.proj_out.act_quantizer\n",
      "leaf: model.output_blocks.4.0.act_quantizer\n",
      "leaf: model.output_blocks.4.0.in_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.4.0.in_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.4.0.in_layers.2.weight_quantizer\n",
      "leaf: model.output_blocks.4.0.in_layers.2.act_quantizer\n",
      "leaf: model.output_blocks.4.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.output_blocks.4.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.output_blocks.4.0.emb_layers.1.act_quantizer\n",
      "leaf: model.output_blocks.4.0.out_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.4.0.out_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.4.0.out_layers.3.weight_quantizer\n",
      "leaf: model.output_blocks.4.0.out_layers.3.act_quantizer\n",
      "no leaf: model.output_blocks.4.0.skip_connection.weight_quantizer\n",
      "leaf: model.output_blocks.4.0.skip_connection.act_quantizer\n",
      "no leaf: model.output_blocks.4.1.proj_in.weight_quantizer\n",
      "leaf: model.output_blocks.4.1.proj_in.act_quantizer\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.output_blocks.4.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.output_blocks.4.1.proj_out.weight_quantizer\n",
      "leaf: model.output_blocks.4.1.proj_out.act_quantizer\n",
      "leaf: model.output_blocks.5.0.act_quantizer\n",
      "leaf: model.output_blocks.5.0.in_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.5.0.in_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.5.0.in_layers.2.weight_quantizer\n",
      "leaf: model.output_blocks.5.0.in_layers.2.act_quantizer\n",
      "leaf: model.output_blocks.5.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.output_blocks.5.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.output_blocks.5.0.emb_layers.1.act_quantizer\n",
      "leaf: model.output_blocks.5.0.out_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.5.0.out_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.5.0.out_layers.3.weight_quantizer\n",
      "leaf: model.output_blocks.5.0.out_layers.3.act_quantizer\n",
      "no leaf: model.output_blocks.5.0.skip_connection.weight_quantizer\n",
      "leaf: model.output_blocks.5.0.skip_connection.act_quantizer\n",
      "no leaf: model.output_blocks.5.1.proj_in.weight_quantizer\n",
      "leaf: model.output_blocks.5.1.proj_in.act_quantizer\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.output_blocks.5.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.output_blocks.5.1.proj_out.weight_quantizer\n",
      "leaf: model.output_blocks.5.1.proj_out.act_quantizer\n",
      "no leaf: model.output_blocks.5.2.conv.weight_quantizer\n",
      "leaf: model.output_blocks.5.2.conv.act_quantizer\n",
      "leaf: model.output_blocks.6.0.act_quantizer\n",
      "leaf: model.output_blocks.6.0.in_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.6.0.in_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.6.0.in_layers.2.weight_quantizer\n",
      "leaf: model.output_blocks.6.0.in_layers.2.act_quantizer\n",
      "leaf: model.output_blocks.6.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.output_blocks.6.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.output_blocks.6.0.emb_layers.1.act_quantizer\n",
      "leaf: model.output_blocks.6.0.out_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.6.0.out_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.6.0.out_layers.3.weight_quantizer\n",
      "leaf: model.output_blocks.6.0.out_layers.3.act_quantizer\n",
      "no leaf: model.output_blocks.6.0.skip_connection.weight_quantizer\n",
      "leaf: model.output_blocks.6.0.skip_connection.act_quantizer\n",
      "no leaf: model.output_blocks.6.1.proj_in.weight_quantizer\n",
      "leaf: model.output_blocks.6.1.proj_in.act_quantizer\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.output_blocks.6.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.output_blocks.6.1.proj_out.weight_quantizer\n",
      "leaf: model.output_blocks.6.1.proj_out.act_quantizer\n",
      "leaf: model.output_blocks.7.0.act_quantizer\n",
      "leaf: model.output_blocks.7.0.in_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.7.0.in_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.7.0.in_layers.2.weight_quantizer\n",
      "leaf: model.output_blocks.7.0.in_layers.2.act_quantizer\n",
      "leaf: model.output_blocks.7.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.output_blocks.7.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.output_blocks.7.0.emb_layers.1.act_quantizer\n",
      "leaf: model.output_blocks.7.0.out_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.7.0.out_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.7.0.out_layers.3.weight_quantizer\n",
      "leaf: model.output_blocks.7.0.out_layers.3.act_quantizer\n",
      "no leaf: model.output_blocks.7.0.skip_connection.weight_quantizer\n",
      "leaf: model.output_blocks.7.0.skip_connection.act_quantizer\n",
      "no leaf: model.output_blocks.7.1.proj_in.weight_quantizer\n",
      "leaf: model.output_blocks.7.1.proj_in.act_quantizer\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.output_blocks.7.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.output_blocks.7.1.proj_out.weight_quantizer\n",
      "leaf: model.output_blocks.7.1.proj_out.act_quantizer\n",
      "leaf: model.output_blocks.8.0.act_quantizer\n",
      "leaf: model.output_blocks.8.0.in_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.8.0.in_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.8.0.in_layers.2.weight_quantizer\n",
      "leaf: model.output_blocks.8.0.in_layers.2.act_quantizer\n",
      "leaf: model.output_blocks.8.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.output_blocks.8.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.output_blocks.8.0.emb_layers.1.act_quantizer\n",
      "leaf: model.output_blocks.8.0.out_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.8.0.out_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.8.0.out_layers.3.weight_quantizer\n",
      "leaf: model.output_blocks.8.0.out_layers.3.act_quantizer\n",
      "no leaf: model.output_blocks.8.0.skip_connection.weight_quantizer\n",
      "leaf: model.output_blocks.8.0.skip_connection.act_quantizer\n",
      "no leaf: model.output_blocks.8.1.proj_in.weight_quantizer\n",
      "leaf: model.output_blocks.8.1.proj_in.act_quantizer\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.output_blocks.8.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.output_blocks.8.1.proj_out.weight_quantizer\n",
      "leaf: model.output_blocks.8.1.proj_out.act_quantizer\n",
      "no leaf: model.output_blocks.8.2.conv.weight_quantizer\n",
      "leaf: model.output_blocks.8.2.conv.act_quantizer\n",
      "leaf: model.output_blocks.9.0.act_quantizer\n",
      "leaf: model.output_blocks.9.0.in_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.9.0.in_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.9.0.in_layers.2.weight_quantizer\n",
      "leaf: model.output_blocks.9.0.in_layers.2.act_quantizer\n",
      "leaf: model.output_blocks.9.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.output_blocks.9.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.output_blocks.9.0.emb_layers.1.act_quantizer\n",
      "leaf: model.output_blocks.9.0.out_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.9.0.out_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.9.0.out_layers.3.weight_quantizer\n",
      "leaf: model.output_blocks.9.0.out_layers.3.act_quantizer\n",
      "no leaf: model.output_blocks.9.0.skip_connection.weight_quantizer\n",
      "leaf: model.output_blocks.9.0.skip_connection.act_quantizer\n",
      "no leaf: model.output_blocks.9.1.proj_in.weight_quantizer\n",
      "leaf: model.output_blocks.9.1.proj_in.act_quantizer\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.output_blocks.9.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.output_blocks.9.1.proj_out.weight_quantizer\n",
      "leaf: model.output_blocks.9.1.proj_out.act_quantizer\n",
      "leaf: model.output_blocks.10.0.act_quantizer\n",
      "leaf: model.output_blocks.10.0.in_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.10.0.in_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.10.0.in_layers.2.weight_quantizer\n",
      "leaf: model.output_blocks.10.0.in_layers.2.act_quantizer\n",
      "leaf: model.output_blocks.10.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.output_blocks.10.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.output_blocks.10.0.emb_layers.1.act_quantizer\n",
      "leaf: model.output_blocks.10.0.out_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.10.0.out_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.10.0.out_layers.3.weight_quantizer\n",
      "leaf: model.output_blocks.10.0.out_layers.3.act_quantizer\n",
      "no leaf: model.output_blocks.10.0.skip_connection.weight_quantizer\n",
      "leaf: model.output_blocks.10.0.skip_connection.act_quantizer\n",
      "no leaf: model.output_blocks.10.1.proj_in.weight_quantizer\n",
      "leaf: model.output_blocks.10.1.proj_in.act_quantizer\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.output_blocks.10.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.output_blocks.10.1.proj_out.weight_quantizer\n",
      "leaf: model.output_blocks.10.1.proj_out.act_quantizer\n",
      "leaf: model.output_blocks.11.0.act_quantizer\n",
      "leaf: model.output_blocks.11.0.in_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.11.0.in_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.11.0.in_layers.2.weight_quantizer\n",
      "leaf: model.output_blocks.11.0.in_layers.2.act_quantizer\n",
      "leaf: model.output_blocks.11.0.emb_layers.0.act_quantizer\n",
      "no leaf: model.output_blocks.11.0.emb_layers.1.weight_quantizer\n",
      "leaf: model.output_blocks.11.0.emb_layers.1.act_quantizer\n",
      "leaf: model.output_blocks.11.0.out_layers.0.act_quantizer\n",
      "leaf: model.output_blocks.11.0.out_layers.1.act_quantizer\n",
      "no leaf: model.output_blocks.11.0.out_layers.3.weight_quantizer\n",
      "leaf: model.output_blocks.11.0.out_layers.3.act_quantizer\n",
      "no leaf: model.output_blocks.11.0.skip_connection.weight_quantizer\n",
      "leaf: model.output_blocks.11.0.skip_connection.act_quantizer\n",
      "no leaf: model.output_blocks.11.1.proj_in.weight_quantizer\n",
      "leaf: model.output_blocks.11.1.proj_in.act_quantizer\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.act_quantizer\n",
      "no leaf: model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn1.act_quantizer_q\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn1.act_quantizer_k\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn1.act_quantizer_v\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn1.act_quantizer_w\n",
      "no leaf: model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.act_quantizer\n",
      "no leaf: model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight_quantizer\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.ff.net.2.act_quantizer\n",
      "no leaf: model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight_quantizer\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.act_quantizer\n",
      "no leaf: model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight_quantizer\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.act_quantizer\n",
      "no leaf: model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight_quantizer\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.act_quantizer\n",
      "no leaf: model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.act_quantizer\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn2.act_quantizer_q\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn2.act_quantizer_k\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn2.act_quantizer_v\n",
      "leaf: model.output_blocks.11.1.transformer_blocks.0.attn2.act_quantizer_w\n",
      "no leaf: model.output_blocks.11.1.proj_out.weight_quantizer\n",
      "leaf: model.output_blocks.11.1.proj_out.act_quantizer\n",
      "leaf: model.out.0.act_quantizer\n",
      "leaf: model.out.1.act_quantizer\n",
      "no leaf: model.out.2.weight_quantizer\n",
      "leaf: model.out.2.act_quantizer\n"
     ]
    }
   ],
   "source": [
    "from qdiff.quant_layer import UniformAffineQuantizer\n",
    "for name, module in qnn.named_modules():\n",
    "    if isinstance(module, UniformAffineQuantizer):\n",
    "        if module.leaf_param:\n",
    "            print('leaf:' ,module.full_name)# = name\n",
    "        else:\n",
    "            print('no leaf:' ,module.full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(qnn.model.input_blocks[1][0].skip_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.input_blocks.1.0\n",
      "model.input_blocks.2.0\n",
      "model.input_blocks.4.0\n",
      "model.input_blocks.5.0\n",
      "model.input_blocks.7.0\n",
      "model.input_blocks.8.0\n",
      "model.input_blocks.10.0\n",
      "model.input_blocks.11.0\n",
      "model.middle_block.0\n",
      "model.middle_block.2\n",
      "model.output_blocks.0.0\n",
      "model.output_blocks.1.0\n",
      "model.output_blocks.2.0\n",
      "model.output_blocks.3.0\n",
      "model.output_blocks.4.0\n",
      "model.output_blocks.5.0\n",
      "model.output_blocks.6.0\n",
      "model.output_blocks.7.0\n",
      "model.output_blocks.8.0\n",
      "model.output_blocks.9.0\n",
      "model.output_blocks.10.0\n",
      "model.output_blocks.11.0\n"
     ]
    }
   ],
   "source": [
    "for module in qnn.modules():\n",
    "    if getattr(module,'skip_connection',False):\n",
    "        print(module.full_name)\n",
    "        #print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(qnn.model.named_children())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsb = list(qnn.model.named_children())[1][1][1][0]#.input_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qdiff.quant_block.QuantResBlock"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rsb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qdiff.quant_layer.QuantOp"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rsb.in_layers[1])#.num_groups, rsb.in_layers[0].num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantOp(\n",
       "  (fwd_func): SiLU()\n",
       "  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "  (activation_function): StraightThrough()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsb.in_layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from ldm.modules.diffusionmodules.util import GroupNorm32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(rsb.in_layers[0],GroupNorm32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading quantized model checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nadavg/q-diffusion/qdiff/utils.py:384: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weight quantization parameters\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for QuantModel:\n\tMissing key(s) in state_dict: \"model.time_embed.0.weight_quantizer.zero_point\", \"model.time_embed.0.weight_quantizer.delta\", \"model.time_embed.2.weight_quantizer.zero_point\", \"model.time_embed.2.weight_quantizer.delta\", \"model.input_blocks.0.0.weight_quantizer.zero_point\", \"model.input_blocks.0.0.weight_quantizer.delta\", \"model.input_blocks.1.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.1.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.1.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.1.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.1.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.1.0.out_layers.3.weight_quantizer.delta\", \"model.input_blocks.1.1.proj_in.weight_quantizer.zero_point\", \"model.input_blocks.1.1.proj_in.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.input_blocks.1.1.proj_out.weight_quantizer.zero_point\", \"model.input_blocks.1.1.proj_out.weight_quantizer.delta\", \"model.input_blocks.2.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.2.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.2.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.2.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.2.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.2.0.out_layers.3.weight_quantizer.delta\", \"model.input_blocks.2.1.proj_in.weight_quantizer.zero_point\", \"model.input_blocks.2.1.proj_in.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.input_blocks.2.1.proj_out.weight_quantizer.zero_point\", \"model.input_blocks.2.1.proj_out.weight_quantizer.delta\", \"model.input_blocks.3.0.op.weight_quantizer.zero_point\", \"model.input_blocks.3.0.op.weight_quantizer.delta\", \"model.input_blocks.4.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.4.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.4.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.4.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.4.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.4.0.out_layers.3.weight_quantizer.delta\", \"model.input_blocks.4.0.skip_connection.weight_quantizer.zero_point\", \"model.input_blocks.4.0.skip_connection.weight_quantizer.delta\", \"model.input_blocks.4.1.proj_in.weight_quantizer.zero_point\", \"model.input_blocks.4.1.proj_in.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.input_blocks.4.1.proj_out.weight_quantizer.zero_point\", \"model.input_blocks.4.1.proj_out.weight_quantizer.delta\", \"model.input_blocks.5.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.5.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.5.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.5.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.5.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.5.0.out_layers.3.weight_quantizer.delta\", \"model.input_blocks.5.1.proj_in.weight_quantizer.zero_point\", \"model.input_blocks.5.1.proj_in.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.input_blocks.5.1.proj_out.weight_quantizer.zero_point\", \"model.input_blocks.5.1.proj_out.weight_quantizer.delta\", \"model.input_blocks.6.0.op.weight_quantizer.zero_point\", \"model.input_blocks.6.0.op.weight_quantizer.delta\", \"model.input_blocks.7.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.7.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.7.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.7.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.7.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.7.0.out_layers.3.weight_quantizer.delta\", \"model.input_blocks.7.0.skip_connection.weight_quantizer.zero_point\", \"model.input_blocks.7.0.skip_connection.weight_quantizer.delta\", \"model.input_blocks.7.1.proj_in.weight_quantizer.zero_point\", \"model.input_blocks.7.1.proj_in.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.input_blocks.7.1.proj_out.weight_quantizer.zero_point\", \"model.input_blocks.7.1.proj_out.weight_quantizer.delta\", \"model.input_blocks.8.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.8.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.8.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.8.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.8.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.8.0.out_layers.3.weight_quantizer.delta\", \"model.input_blocks.8.1.proj_in.weight_quantizer.zero_point\", \"model.input_blocks.8.1.proj_in.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.input_blocks.8.1.proj_out.weight_quantizer.zero_point\", \"model.input_blocks.8.1.proj_out.weight_quantizer.delta\", \"model.input_blocks.9.0.op.weight_quantizer.zero_point\", \"model.input_blocks.9.0.op.weight_quantizer.delta\", \"model.input_blocks.10.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.10.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.10.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.10.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.10.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.10.0.out_layers.3.weight_quantizer.delta\", \"model.input_blocks.11.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.11.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.11.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.11.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.11.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.11.0.out_layers.3.weight_quantizer.delta\", \"model.middle_block.0.in_layers.2.weight_quantizer.zero_point\", \"model.middle_block.0.in_layers.2.weight_quantizer.delta\", \"model.middle_block.0.emb_layers.1.weight_quantizer.zero_point\", \"model.middle_block.0.emb_layers.1.weight_quantizer.delta\", \"model.middle_block.0.out_layers.3.weight_quantizer.zero_point\", \"model.middle_block.0.out_layers.3.weight_quantizer.delta\", \"model.middle_block.1.proj_in.weight_quantizer.zero_point\", \"model.middle_block.1.proj_in.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.middle_block.1.proj_out.weight_quantizer.zero_point\", \"model.middle_block.1.proj_out.weight_quantizer.delta\", \"model.middle_block.2.in_layers.2.weight_quantizer.zero_point\", \"model.middle_block.2.in_layers.2.weight_quantizer.delta\", \"model.middle_block.2.emb_layers.1.weight_quantizer.zero_point\", \"model.middle_block.2.emb_layers.1.weight_quantizer.delta\", \"model.middle_block.2.out_layers.3.weight_quantizer.zero_point\", \"model.middle_block.2.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.0.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.0.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.0.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.0.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.0.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.0.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.0.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.0.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.0.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.0.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.1.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.1.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.1.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.1.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.1.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.1.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.1.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.1.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.1.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.1.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.2.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.2.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.2.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.2.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.2.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.2.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.2.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.2.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.2.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.2.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.2.1.conv.weight_quantizer.zero_point\", \"model.output_blocks.2.1.conv.weight_quantizer.delta\", \"model.output_blocks.3.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.3.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.3.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.3.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.3.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.3.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.3.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.3.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.3.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.3.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.3.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.3.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.3.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.3.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.4.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.4.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.4.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.4.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.4.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.4.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.4.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.4.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.4.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.4.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.4.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.4.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.4.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.4.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.5.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.5.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.5.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.5.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.5.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.5.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.5.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.5.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.5.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.5.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.5.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.5.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.5.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.5.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.5.2.conv.weight_quantizer.zero_point\", \"model.output_blocks.5.2.conv.weight_quantizer.delta\", \"model.output_blocks.6.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.6.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.6.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.6.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.6.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.6.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.6.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.6.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.6.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.6.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.6.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.6.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.6.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.6.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.7.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.7.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.7.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.7.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.7.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.7.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.7.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.7.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.7.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.7.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.7.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.7.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.7.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.7.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.8.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.8.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.8.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.8.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.8.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.8.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.8.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.8.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.8.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.8.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.8.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.8.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.8.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.8.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.8.2.conv.weight_quantizer.zero_point\", \"model.output_blocks.8.2.conv.weight_quantizer.delta\", \"model.output_blocks.9.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.9.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.9.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.9.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.9.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.9.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.9.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.9.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.9.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.9.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.9.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.9.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.9.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.9.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.10.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.10.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.10.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.10.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.10.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.10.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.10.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.10.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.10.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.10.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.10.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.10.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.10.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.10.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.11.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.11.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.11.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.11.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.11.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.11.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.11.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.11.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.11.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.11.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.11.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.11.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.11.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.11.1.proj_out.weight_quantizer.delta\", \"model.out.2.weight_quantizer.zero_point\", \"model.out.2.weight_quantizer.delta\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m cali_data \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m), torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1000\u001b[39m, (\u001b[38;5;241m1\u001b[39m,)), torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m77\u001b[39m, \u001b[38;5;241m768\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#resume_cali_model(qnn, cali_ckpt, cali_data, False, \"qdiff\", cond=True)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mresume_cali_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_ckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcali_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqdiff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/q-diffusion/qdiff/utils.py:406\u001b[0m, in \u001b[0;36mresume_cali_model\u001b[0;34m(qnn, ckpt_path, cali_data, quant_act, act_quant_mode, cond)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m ckpt[key]\n\u001b[0;32m--> 406\u001b[0m \u001b[43mqnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mact_quant_mode\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqdiff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m qnn\u001b[38;5;241m.\u001b[39mset_quant_state(weight_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, act_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m qnn\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodules():\n",
      "File \u001b[0;32m~/anaconda3/envs/qdiff/lib/python3.8/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for QuantModel:\n\tMissing key(s) in state_dict: \"model.time_embed.0.weight_quantizer.zero_point\", \"model.time_embed.0.weight_quantizer.delta\", \"model.time_embed.2.weight_quantizer.zero_point\", \"model.time_embed.2.weight_quantizer.delta\", \"model.input_blocks.0.0.weight_quantizer.zero_point\", \"model.input_blocks.0.0.weight_quantizer.delta\", \"model.input_blocks.1.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.1.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.1.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.1.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.1.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.1.0.out_layers.3.weight_quantizer.delta\", \"model.input_blocks.1.1.proj_in.weight_quantizer.zero_point\", \"model.input_blocks.1.1.proj_in.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.input_blocks.1.1.proj_out.weight_quantizer.zero_point\", \"model.input_blocks.1.1.proj_out.weight_quantizer.delta\", \"model.input_blocks.2.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.2.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.2.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.2.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.2.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.2.0.out_layers.3.weight_quantizer.delta\", \"model.input_blocks.2.1.proj_in.weight_quantizer.zero_point\", \"model.input_blocks.2.1.proj_in.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.input_blocks.2.1.proj_out.weight_quantizer.zero_point\", \"model.input_blocks.2.1.proj_out.weight_quantizer.delta\", \"model.input_blocks.3.0.op.weight_quantizer.zero_point\", \"model.input_blocks.3.0.op.weight_quantizer.delta\", \"model.input_blocks.4.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.4.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.4.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.4.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.4.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.4.0.out_layers.3.weight_quantizer.delta\", \"model.input_blocks.4.0.skip_connection.weight_quantizer.zero_point\", \"model.input_blocks.4.0.skip_connection.weight_quantizer.delta\", \"model.input_blocks.4.1.proj_in.weight_quantizer.zero_point\", \"model.input_blocks.4.1.proj_in.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.input_blocks.4.1.proj_out.weight_quantizer.zero_point\", \"model.input_blocks.4.1.proj_out.weight_quantizer.delta\", \"model.input_blocks.5.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.5.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.5.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.5.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.5.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.5.0.out_layers.3.weight_quantizer.delta\", \"model.input_blocks.5.1.proj_in.weight_quantizer.zero_point\", \"model.input_blocks.5.1.proj_in.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.input_blocks.5.1.proj_out.weight_quantizer.zero_point\", \"model.input_blocks.5.1.proj_out.weight_quantizer.delta\", \"model.input_blocks.6.0.op.weight_quantizer.zero_point\", \"model.input_blocks.6.0.op.weight_quantizer.delta\", \"model.input_blocks.7.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.7.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.7.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.7.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.7.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.7.0.out_layers.3.weight_quantizer.delta\", \"model.input_blocks.7.0.skip_connection.weight_quantizer.zero_point\", \"model.input_blocks.7.0.skip_connection.weight_quantizer.delta\", \"model.input_blocks.7.1.proj_in.weight_quantizer.zero_point\", \"model.input_blocks.7.1.proj_in.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.input_blocks.7.1.proj_out.weight_quantizer.zero_point\", \"model.input_blocks.7.1.proj_out.weight_quantizer.delta\", \"model.input_blocks.8.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.8.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.8.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.8.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.8.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.8.0.out_layers.3.weight_quantizer.delta\", \"model.input_blocks.8.1.proj_in.weight_quantizer.zero_point\", \"model.input_blocks.8.1.proj_in.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.input_blocks.8.1.proj_out.weight_quantizer.zero_point\", \"model.input_blocks.8.1.proj_out.weight_quantizer.delta\", \"model.input_blocks.9.0.op.weight_quantizer.zero_point\", \"model.input_blocks.9.0.op.weight_quantizer.delta\", \"model.input_blocks.10.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.10.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.10.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.10.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.10.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.10.0.out_layers.3.weight_quantizer.delta\", \"model.input_blocks.11.0.in_layers.2.weight_quantizer.zero_point\", \"model.input_blocks.11.0.in_layers.2.weight_quantizer.delta\", \"model.input_blocks.11.0.emb_layers.1.weight_quantizer.zero_point\", \"model.input_blocks.11.0.emb_layers.1.weight_quantizer.delta\", \"model.input_blocks.11.0.out_layers.3.weight_quantizer.zero_point\", \"model.input_blocks.11.0.out_layers.3.weight_quantizer.delta\", \"model.middle_block.0.in_layers.2.weight_quantizer.zero_point\", \"model.middle_block.0.in_layers.2.weight_quantizer.delta\", \"model.middle_block.0.emb_layers.1.weight_quantizer.zero_point\", \"model.middle_block.0.emb_layers.1.weight_quantizer.delta\", \"model.middle_block.0.out_layers.3.weight_quantizer.zero_point\", \"model.middle_block.0.out_layers.3.weight_quantizer.delta\", \"model.middle_block.1.proj_in.weight_quantizer.zero_point\", \"model.middle_block.1.proj_in.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.middle_block.1.proj_out.weight_quantizer.zero_point\", \"model.middle_block.1.proj_out.weight_quantizer.delta\", \"model.middle_block.2.in_layers.2.weight_quantizer.zero_point\", \"model.middle_block.2.in_layers.2.weight_quantizer.delta\", \"model.middle_block.2.emb_layers.1.weight_quantizer.zero_point\", \"model.middle_block.2.emb_layers.1.weight_quantizer.delta\", \"model.middle_block.2.out_layers.3.weight_quantizer.zero_point\", \"model.middle_block.2.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.0.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.0.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.0.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.0.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.0.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.0.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.0.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.0.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.0.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.0.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.1.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.1.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.1.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.1.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.1.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.1.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.1.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.1.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.1.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.1.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.2.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.2.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.2.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.2.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.2.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.2.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.2.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.2.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.2.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.2.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.2.1.conv.weight_quantizer.zero_point\", \"model.output_blocks.2.1.conv.weight_quantizer.delta\", \"model.output_blocks.3.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.3.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.3.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.3.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.3.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.3.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.3.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.3.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.3.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.3.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.3.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.3.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.3.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.3.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.4.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.4.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.4.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.4.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.4.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.4.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.4.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.4.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.4.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.4.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.4.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.4.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.4.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.4.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.5.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.5.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.5.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.5.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.5.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.5.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.5.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.5.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.5.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.5.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.5.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.5.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.5.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.5.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.5.2.conv.weight_quantizer.zero_point\", \"model.output_blocks.5.2.conv.weight_quantizer.delta\", \"model.output_blocks.6.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.6.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.6.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.6.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.6.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.6.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.6.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.6.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.6.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.6.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.6.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.6.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.6.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.6.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.7.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.7.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.7.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.7.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.7.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.7.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.7.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.7.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.7.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.7.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.7.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.7.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.7.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.7.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.8.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.8.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.8.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.8.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.8.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.8.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.8.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.8.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.8.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.8.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.8.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.8.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.8.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.8.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.8.2.conv.weight_quantizer.zero_point\", \"model.output_blocks.8.2.conv.weight_quantizer.delta\", \"model.output_blocks.9.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.9.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.9.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.9.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.9.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.9.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.9.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.9.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.9.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.9.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.9.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.9.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.9.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.9.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.10.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.10.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.10.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.10.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.10.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.10.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.10.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.10.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.10.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.10.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.10.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.10.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.10.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.10.1.proj_out.weight_quantizer.delta\", \"model.output_blocks.11.0.in_layers.2.weight_quantizer.zero_point\", \"model.output_blocks.11.0.in_layers.2.weight_quantizer.delta\", \"model.output_blocks.11.0.emb_layers.1.weight_quantizer.zero_point\", \"model.output_blocks.11.0.emb_layers.1.weight_quantizer.delta\", \"model.output_blocks.11.0.out_layers.3.weight_quantizer.zero_point\", \"model.output_blocks.11.0.out_layers.3.weight_quantizer.delta\", \"model.output_blocks.11.0.skip_connection.weight_quantizer.zero_point\", \"model.output_blocks.11.0.skip_connection.weight_quantizer.delta\", \"model.output_blocks.11.0.skip_connection.weight_quantizer_0.zero_point\", \"model.output_blocks.11.0.skip_connection.weight_quantizer_0.delta\", \"model.output_blocks.11.1.proj_in.weight_quantizer.zero_point\", \"model.output_blocks.11.1.proj_in.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight_quantizer.delta\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.zero_point\", \"model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight_quantizer.delta\", \"model.output_blocks.11.1.proj_out.weight_quantizer.zero_point\", \"model.output_blocks.11.1.proj_out.weight_quantizer.delta\", \"model.out.2.weight_quantizer.zero_point\", \"model.out.2.weight_quantizer.delta\". "
     ]
    }
   ],
   "source": [
    "cali_data = (torch.randn(1, 4, 64, 64), torch.randint(0, 1000, (1,)), torch.randn(1, 77, 768))\n",
    "#resume_cali_model(qnn, cali_ckpt, cali_data, False, \"qdiff\", cond=True)\n",
    "resume_cali_model(qnn, weight_ckpt, cali_data, False, \"qdiff\", cond=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampler.model.model.diffusion_model = qnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ldm.modules.diffusionmodules.openaimodel.UNetModel"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(qnn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = model.model.diffusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "qunet = qnn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 3, 12)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qunet.input_blocks),len(qunet.middle_block),len(qunet.output_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = lambda l : str(type(l)).split('.')[-1].split('\\'')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['QuantModule'],\n",
       " ['QuantResBlock', 'SpatialTransformer'],\n",
       " ['QuantResBlock', 'SpatialTransformer'],\n",
       " ['Downsample'],\n",
       " ['QuantResBlock', 'SpatialTransformer'],\n",
       " ['QuantResBlock', 'SpatialTransformer'],\n",
       " ['Downsample'],\n",
       " ['QuantResBlock', 'SpatialTransformer'],\n",
       " ['QuantResBlock', 'SpatialTransformer'],\n",
       " ['Downsample'],\n",
       " ['QuantResBlock'],\n",
       " ['QuantResBlock']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_b =[]\n",
    "for i,inb_i in enumerate(qunet.input_blocks):\n",
    "    in_b.append([])\n",
    "    for l in inb_i:\n",
    "        in_b[i].append(lt(l))\n",
    "in_b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QuantResBlock', 'SpatialTransformer', 'QuantResBlock']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_b=[]\n",
    "for l in qunet.middle_block:\n",
    "    in_b.append(lt(l))\n",
    "in_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['QuantResBlock'],\n",
       " ['QuantResBlock'],\n",
       " ['QuantResBlock', 'Upsample'],\n",
       " ['QuantResBlock', 'SpatialTransformer'],\n",
       " ['QuantResBlock', 'SpatialTransformer'],\n",
       " ['QuantResBlock', 'SpatialTransformer', 'Upsample'],\n",
       " ['QuantResBlock', 'SpatialTransformer'],\n",
       " ['QuantResBlock', 'SpatialTransformer'],\n",
       " ['QuantResBlock', 'SpatialTransformer', 'Upsample'],\n",
       " ['QuantResBlock', 'SpatialTransformer'],\n",
       " ['QuantResBlock', 'SpatialTransformer'],\n",
       " ['QuantResBlock', 'SpatialTransformer']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_b =[]\n",
    "for i,inb_i in enumerate(qunet.output_blocks):\n",
    "    in_b.append([])\n",
    "    for l in inb_i:\n",
    "        in_b[i].append(lt(l))\n",
    "in_b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(qdiff.quant_layer.QuantModule,\n",
       " qdiff.quant_block.QuantResBlock,\n",
       " ldm.modules.diffusionmodules.openaimodel.Downsample,\n",
       " ldm.modules.diffusionmodules.openaimodel.Upsample,\n",
       " ldm.modules.attention.SpatialTransformer,\n",
       " qdiff.quant_block.QuantResBlock)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2d = qunet.input_blocks[0][0]\n",
    "rb = qunet.input_blocks[1][0]\n",
    "us = qunet.output_blocks[2][1]\n",
    "ds = qunet.input_blocks[3][0]\n",
    "st = qunet.input_blocks[1][1]\n",
    "rb_u = qunet.output_blocks[0][0]\n",
    "type(c2d),type(rb),type(ds),type(us),type(st),type(rb_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model.input_blocks.1.0', qdiff.quant_block.QuantResBlock)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.full_name,type(rb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantOp(\n",
       "    (fwd_func): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "    (activation_function): StraightThrough()\n",
       "  )\n",
       "  (1): QuantOp(\n",
       "    (fwd_func): SiLU()\n",
       "    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "    (activation_function): StraightThrough()\n",
       "  )\n",
       "  (2): QuantModule(\n",
       "    320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=True, channel_wise=True, leaf_param=False)\n",
       "    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "    (activation_function): StraightThrough()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.in_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantOp(\n",
       "    (fwd_func): SiLU()\n",
       "    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "    (activation_function): StraightThrough()\n",
       "  )\n",
       "  (1): QuantModule(\n",
       "    in_features=1280, out_features=320, bias=True\n",
       "    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=True, channel_wise=True, leaf_param=False)\n",
       "    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "    (activation_function): StraightThrough()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.emb_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qunet.output_blocks[0][0].in_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.updown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "  (1): SiLU()\n",
       "  (2): QuantModule(\n",
       "    320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "    (weight_quantizer): AdaRoundQuantizer(bit=8, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "    (activation_function): StraightThrough()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.in_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StraightThrough()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.in_layers[2].activation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): SiLU()\n",
       "  (1): QuantModule(\n",
       "    in_features=1280, out_features=320, bias=True\n",
       "    (weight_quantizer): AdaRoundQuantizer(bit=8, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "    (activation_function): StraightThrough()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.emb_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "  (1): SiLU()\n",
       "  (2): Dropout(p=0, inplace=False)\n",
       "  (3): QuantModule(\n",
       "    320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "    (weight_quantizer): AdaRoundQuantizer(bit=8, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "    (activation_function): StraightThrough()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.out_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downsample(\n",
       "  (op): QuantModule(\n",
       "    320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "    (weight_quantizer): AdaRoundQuantizer(bit=8, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "    (activation_function): StraightThrough()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ldm.modules.attention.SpatialTransformer"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroupNorm(32, 320, eps=1e-06, affine=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantModule(\n",
       "  320, 320, kernel_size=(1, 1), stride=(1, 1)\n",
       "  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "  (activation_function): StraightThrough()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.proj_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossAttention(\n",
       "  (to_q): QuantModule(\n",
       "    in_features=320, out_features=320, bias=False\n",
       "    (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "    (activation_function): StraightThrough()\n",
       "  )\n",
       "  (to_k): QuantModule(\n",
       "    in_features=320, out_features=320, bias=False\n",
       "    (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "    (activation_function): StraightThrough()\n",
       "  )\n",
       "  (to_v): QuantModule(\n",
       "    in_features=320, out_features=320, bias=False\n",
       "    (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "    (activation_function): StraightThrough()\n",
       "  )\n",
       "  (qk_matmul): CrossQKMatMul()\n",
       "  (smv_matmul): CrossSMVMatMul()\n",
       "  (to_out): Sequential(\n",
       "    (0): QuantModule(\n",
       "      in_features=320, out_features=320, bias=True\n",
       "      (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "      (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "      (activation_function): StraightThrough()\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "  (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "  (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "  (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.transformer_blocks[0].attn1\n",
    "#.qk_matmul#.qk_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantModule(\n",
       "  2560, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "  (activation_function): StraightThrough()\n",
       "  (weight_quantizer_0): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "  (act_quantizer_0): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb_u.skip_connection#.updown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.shape = torch.Size([3, 4, 64, 64])\n",
    "\n",
    "t.shape = torch.Size([3]) \n",
    "\n",
    "c.shape = torch.Size([3, 77, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNetModel(\n",
       "  (time_embed): Sequential(\n",
       "    (0): QuantModule(\n",
       "      in_features=320, out_features=1280, bias=True\n",
       "      (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "      (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "      (activation_function): StraightThrough()\n",
       "    )\n",
       "    (1): SiLU()\n",
       "    (2): QuantModule(\n",
       "      in_features=1280, out_features=1280, bias=True\n",
       "      (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "      (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "      (activation_function): StraightThrough()\n",
       "    )\n",
       "  )\n",
       "  (input_blocks): ModuleList(\n",
       "    (0): TimestepEmbedSequential(\n",
       "      (0): QuantModule(\n",
       "        4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "      )\n",
       "    )\n",
       "    (1-2): 2 x TimestepEmbedSequential(\n",
       "      (0): QuantResBlock(\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): QuantModule(\n",
       "            320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): QuantModule(\n",
       "            in_features=1280, out_features=320, bias=True\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): QuantModule(\n",
       "            320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): QuantModule(\n",
       "          320, 320, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): QuantBasicTransformerBlock(\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=320, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=320, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=320, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=320, out_features=320, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): QuantModule(\n",
       "                    in_features=320, out_features=2560, bias=True\n",
       "                    (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                    (activation_function): StraightThrough()\n",
       "                  )\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): QuantModule(\n",
       "                  in_features=1280, out_features=320, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=320, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=768, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=768, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=320, out_features=320, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): QuantModule(\n",
       "          320, 320, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): QuantModule(\n",
       "          320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TimestepEmbedSequential(\n",
       "      (0): QuantResBlock(\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): QuantModule(\n",
       "            320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): QuantModule(\n",
       "            in_features=1280, out_features=640, bias=True\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): QuantModule(\n",
       "            640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (skip_connection): QuantModule(\n",
       "          320, 640, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): QuantModule(\n",
       "          640, 640, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): QuantBasicTransformerBlock(\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=640, out_features=640, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): QuantModule(\n",
       "                    in_features=640, out_features=5120, bias=True\n",
       "                    (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                    (activation_function): StraightThrough()\n",
       "                  )\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): QuantModule(\n",
       "                  in_features=2560, out_features=640, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=768, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=768, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=640, out_features=640, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): QuantModule(\n",
       "          640, 640, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): QuantResBlock(\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): QuantModule(\n",
       "            640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): QuantModule(\n",
       "            in_features=1280, out_features=640, bias=True\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): QuantModule(\n",
       "            640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): QuantModule(\n",
       "          640, 640, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): QuantBasicTransformerBlock(\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=640, out_features=640, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): QuantModule(\n",
       "                    in_features=640, out_features=5120, bias=True\n",
       "                    (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                    (activation_function): StraightThrough()\n",
       "                  )\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): QuantModule(\n",
       "                  in_features=2560, out_features=640, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=768, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=768, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=640, out_features=640, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): QuantModule(\n",
       "          640, 640, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): QuantModule(\n",
       "          640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): QuantResBlock(\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): QuantModule(\n",
       "            640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): QuantModule(\n",
       "            in_features=1280, out_features=1280, bias=True\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): QuantModule(\n",
       "            1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (skip_connection): QuantModule(\n",
       "          640, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): QuantModule(\n",
       "          1280, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): QuantBasicTransformerBlock(\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=1280, out_features=1280, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): QuantModule(\n",
       "                    in_features=1280, out_features=10240, bias=True\n",
       "                    (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                    (activation_function): StraightThrough()\n",
       "                  )\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): QuantModule(\n",
       "                  in_features=5120, out_features=1280, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=768, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=768, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=1280, out_features=1280, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): QuantModule(\n",
       "          1280, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): QuantResBlock(\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): QuantModule(\n",
       "            1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): QuantModule(\n",
       "            in_features=1280, out_features=1280, bias=True\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): QuantModule(\n",
       "            1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): QuantModule(\n",
       "          1280, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): QuantBasicTransformerBlock(\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=1280, out_features=1280, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): QuantModule(\n",
       "                    in_features=1280, out_features=10240, bias=True\n",
       "                    (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                    (activation_function): StraightThrough()\n",
       "                  )\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): QuantModule(\n",
       "                  in_features=5120, out_features=1280, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=768, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=768, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=1280, out_features=1280, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): QuantModule(\n",
       "          1280, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): QuantModule(\n",
       "          1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10-11): 2 x TimestepEmbedSequential(\n",
       "      (0): QuantResBlock(\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): QuantModule(\n",
       "            1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): QuantModule(\n",
       "            in_features=1280, out_features=1280, bias=True\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): QuantModule(\n",
       "            1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (middle_block): TimestepEmbedSequential(\n",
       "    (0): QuantResBlock(\n",
       "      (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "      (activation_function): StraightThrough()\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): QuantModule(\n",
       "          1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): QuantModule(\n",
       "          in_features=1280, out_features=1280, bias=True\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): QuantModule(\n",
       "          1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): SpatialTransformer(\n",
       "      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "      (proj_in): QuantModule(\n",
       "        1280, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "      )\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): QuantBasicTransformerBlock(\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "          (attn1): CrossAttention(\n",
       "            (to_q): QuantModule(\n",
       "              in_features=1280, out_features=1280, bias=False\n",
       "              (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "              (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (activation_function): StraightThrough()\n",
       "            )\n",
       "            (to_k): QuantModule(\n",
       "              in_features=1280, out_features=1280, bias=False\n",
       "              (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "              (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (activation_function): StraightThrough()\n",
       "            )\n",
       "            (to_v): QuantModule(\n",
       "              in_features=1280, out_features=1280, bias=False\n",
       "              (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "              (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (activation_function): StraightThrough()\n",
       "            )\n",
       "            (qk_matmul): CrossQKMatMul()\n",
       "            (smv_matmul): CrossSMVMatMul()\n",
       "            (to_out): Sequential(\n",
       "              (0): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=True\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): GEGLU(\n",
       "                (proj): QuantModule(\n",
       "                  in_features=1280, out_features=10240, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): QuantModule(\n",
       "                in_features=5120, out_features=1280, bias=True\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn2): CrossAttention(\n",
       "            (to_q): QuantModule(\n",
       "              in_features=1280, out_features=1280, bias=False\n",
       "              (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "              (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (activation_function): StraightThrough()\n",
       "            )\n",
       "            (to_k): QuantModule(\n",
       "              in_features=768, out_features=1280, bias=False\n",
       "              (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "              (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (activation_function): StraightThrough()\n",
       "            )\n",
       "            (to_v): QuantModule(\n",
       "              in_features=768, out_features=1280, bias=False\n",
       "              (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "              (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (activation_function): StraightThrough()\n",
       "            )\n",
       "            (qk_matmul): CrossQKMatMul()\n",
       "            (smv_matmul): CrossSMVMatMul()\n",
       "            (to_out): Sequential(\n",
       "              (0): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=True\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): QuantModule(\n",
       "        1280, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "      )\n",
       "    )\n",
       "    (2): QuantResBlock(\n",
       "      (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "      (activation_function): StraightThrough()\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): QuantModule(\n",
       "          1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): QuantModule(\n",
       "          in_features=1280, out_features=1280, bias=True\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): QuantModule(\n",
       "          1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (output_blocks): ModuleList(\n",
       "    (0-1): 2 x TimestepEmbedSequential(\n",
       "      (0): QuantResBlock(\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): QuantModule(\n",
       "            2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): QuantModule(\n",
       "            in_features=1280, out_features=1280, bias=True\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): QuantModule(\n",
       "            1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (skip_connection): QuantModule(\n",
       "          2560, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "          (weight_quantizer_0): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer_0): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TimestepEmbedSequential(\n",
       "      (0): QuantResBlock(\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): QuantModule(\n",
       "            2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): QuantModule(\n",
       "            in_features=1280, out_features=1280, bias=True\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): QuantModule(\n",
       "            1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (skip_connection): QuantModule(\n",
       "          2560, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "          (weight_quantizer_0): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer_0): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Upsample(\n",
       "        (conv): QuantModule(\n",
       "          1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3-4): 2 x TimestepEmbedSequential(\n",
       "      (0): QuantResBlock(\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): QuantModule(\n",
       "            2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): QuantModule(\n",
       "            in_features=1280, out_features=1280, bias=True\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): QuantModule(\n",
       "            1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (skip_connection): QuantModule(\n",
       "          2560, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "          (weight_quantizer_0): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer_0): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): QuantModule(\n",
       "          1280, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): QuantBasicTransformerBlock(\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=1280, out_features=1280, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): QuantModule(\n",
       "                    in_features=1280, out_features=10240, bias=True\n",
       "                    (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                    (activation_function): StraightThrough()\n",
       "                  )\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): QuantModule(\n",
       "                  in_features=5120, out_features=1280, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=768, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=768, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=1280, out_features=1280, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): QuantModule(\n",
       "          1280, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): QuantResBlock(\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): QuantModule(\n",
       "            1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): QuantModule(\n",
       "            in_features=1280, out_features=1280, bias=True\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): QuantModule(\n",
       "            1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (skip_connection): QuantModule(\n",
       "          1920, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "          (weight_quantizer_0): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer_0): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): QuantModule(\n",
       "          1280, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): QuantBasicTransformerBlock(\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=1280, out_features=1280, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): QuantModule(\n",
       "                    in_features=1280, out_features=10240, bias=True\n",
       "                    (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                    (activation_function): StraightThrough()\n",
       "                  )\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): QuantModule(\n",
       "                  in_features=5120, out_features=1280, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=1280, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=768, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=768, out_features=1280, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=1280, out_features=1280, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): QuantModule(\n",
       "          1280, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "      (2): Upsample(\n",
       "        (conv): QuantModule(\n",
       "          1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): QuantResBlock(\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): QuantModule(\n",
       "            1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): QuantModule(\n",
       "            in_features=1280, out_features=640, bias=True\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): QuantModule(\n",
       "            640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (skip_connection): QuantModule(\n",
       "          1920, 640, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "          (weight_quantizer_0): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer_0): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): QuantModule(\n",
       "          640, 640, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): QuantBasicTransformerBlock(\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=640, out_features=640, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): QuantModule(\n",
       "                    in_features=640, out_features=5120, bias=True\n",
       "                    (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                    (activation_function): StraightThrough()\n",
       "                  )\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): QuantModule(\n",
       "                  in_features=2560, out_features=640, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=768, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=768, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=640, out_features=640, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): QuantModule(\n",
       "          640, 640, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): QuantResBlock(\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): QuantModule(\n",
       "            1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): QuantModule(\n",
       "            in_features=1280, out_features=640, bias=True\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): QuantModule(\n",
       "            640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (skip_connection): QuantModule(\n",
       "          1280, 640, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "          (weight_quantizer_0): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer_0): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): QuantModule(\n",
       "          640, 640, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): QuantBasicTransformerBlock(\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=640, out_features=640, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): QuantModule(\n",
       "                    in_features=640, out_features=5120, bias=True\n",
       "                    (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                    (activation_function): StraightThrough()\n",
       "                  )\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): QuantModule(\n",
       "                  in_features=2560, out_features=640, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=768, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=768, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=640, out_features=640, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): QuantModule(\n",
       "          640, 640, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): QuantResBlock(\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): QuantModule(\n",
       "            960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): QuantModule(\n",
       "            in_features=1280, out_features=640, bias=True\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): QuantModule(\n",
       "            640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (skip_connection): QuantModule(\n",
       "          960, 640, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "          (weight_quantizer_0): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer_0): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): QuantModule(\n",
       "          640, 640, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): QuantBasicTransformerBlock(\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=640, out_features=640, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): QuantModule(\n",
       "                    in_features=640, out_features=5120, bias=True\n",
       "                    (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                    (activation_function): StraightThrough()\n",
       "                  )\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): QuantModule(\n",
       "                  in_features=2560, out_features=640, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=640, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=768, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=768, out_features=640, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=640, out_features=640, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): QuantModule(\n",
       "          640, 640, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "      (2): Upsample(\n",
       "        (conv): QuantModule(\n",
       "          640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): QuantResBlock(\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): QuantModule(\n",
       "            960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): QuantModule(\n",
       "            in_features=1280, out_features=320, bias=True\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): QuantModule(\n",
       "            320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (skip_connection): QuantModule(\n",
       "          960, 320, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "          (weight_quantizer_0): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer_0): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): QuantModule(\n",
       "          320, 320, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): QuantBasicTransformerBlock(\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=320, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=320, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=320, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=320, out_features=320, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): QuantModule(\n",
       "                    in_features=320, out_features=2560, bias=True\n",
       "                    (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                    (activation_function): StraightThrough()\n",
       "                  )\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): QuantModule(\n",
       "                  in_features=1280, out_features=320, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=320, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=768, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=768, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=320, out_features=320, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): QuantModule(\n",
       "          320, 320, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10-11): 2 x TimestepEmbedSequential(\n",
       "      (0): QuantResBlock(\n",
       "        (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        (activation_function): StraightThrough()\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): QuantModule(\n",
       "            640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): QuantModule(\n",
       "            in_features=1280, out_features=320, bias=True\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): QuantModule(\n",
       "            320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "          )\n",
       "        )\n",
       "        (skip_connection): QuantModule(\n",
       "          640, 320, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "          (weight_quantizer_0): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer_0): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): QuantModule(\n",
       "          320, 320, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): QuantBasicTransformerBlock(\n",
       "            (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            (activation_function): StraightThrough()\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=320, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=320, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=320, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=320, out_features=320, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): QuantModule(\n",
       "                    in_features=320, out_features=2560, bias=True\n",
       "                    (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                    (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                    (activation_function): StraightThrough()\n",
       "                  )\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): QuantModule(\n",
       "                  in_features=1280, out_features=320, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): QuantModule(\n",
       "                in_features=320, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_k): QuantModule(\n",
       "                in_features=768, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (to_v): QuantModule(\n",
       "                in_features=768, out_features=320, bias=False\n",
       "                (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                (activation_function): StraightThrough()\n",
       "              )\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): QuantModule(\n",
       "                  in_features=320, out_features=320, bias=True\n",
       "                  (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "                  (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "                  (activation_function): StraightThrough()\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (act_quantizer_q): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_k): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_v): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "              (act_quantizer_w): UniformAffineQuantizer(bit=16, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): QuantModule(\n",
       "          320, 320, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "          (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "          (activation_function): StraightThrough()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "    (1): SiLU()\n",
       "    (2): QuantModule(\n",
       "      320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "      (weight_quantizer): AdaRoundQuantizer(bit=4, symmetric=False, round_mode=learned_hard_sigmoid)\n",
       "      (act_quantizer): UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)\n",
       "      (activation_function): StraightThrough()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=qnn.cuda()\n",
    "_=qnn.model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapes = [[3, 4, 64, 64],[3],[3,77,768]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([3, 4, 64, 64]), torch.Size([3]), torch.Size([3, 77, 768])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device='cuda'#'cpu'\n",
    "input_data = [torch.randn(input_shape).to(device) for input_shape in input_shapes]\n",
    "[input_data[0].shape,input_data[1].shape,input_data[2].shape]\n",
    "#input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm.modules.diffusionmodules.openaimodel import ResBlock\n",
    "from qdiff.quant_block import QuantResBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn.set_quant_state(True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,module in qnn.named_modules():\n",
    "    if isinstance(module,QuantModule,Quant):\n",
    "        print(name,module.full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cali_data = (torch.randn(1, 4, 64, 64), torch.randint(0, 1000, (1,)), torch.randn(1, 77, 768))\n",
    "#resume_cali_model(qnn, opt.cali_ckpt, cali_data, opt.quant_act, \"qdiff\", cond=opt.cond)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm.modules.diffusionmodules.util import timestep_embedding\n",
    "t_emb = timestep_embedding(input_data[1],qnn.model.model_channels, repeat_only=False)\n",
    "emb = qnn.model.time_embed(t_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 320, 64, 64])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_rs1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_rs1 = c2d(input_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, torch.Size([3, 320, 64, 64]), qdiff.quant_layer.QuantModule)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(in_rs1),in_rs1.shape,type(c2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identity()"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.skip_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-6.0483e-01,  9.5565e-02,  6.0763e-02,  ..., -7.1485e-02,\n",
       "           -7.0646e-01,  3.4055e-01],\n",
       "          [ 6.4123e-01, -6.7859e-01, -3.0940e-01,  ...,  7.4715e-02,\n",
       "           -3.9171e-01, -1.1731e+00],\n",
       "          [-3.2907e-01,  4.4286e-01, -1.7953e-01,  ..., -7.3238e-02,\n",
       "           -1.1173e-01, -4.3877e-01],\n",
       "          ...,\n",
       "          [ 1.4083e-02, -1.8868e-01,  1.3839e-01,  ..., -3.5201e-02,\n",
       "            4.0740e-01, -2.2696e-01],\n",
       "          [ 5.6219e-01, -1.6915e-01, -6.0696e-01,  ..., -9.1107e-02,\n",
       "           -1.1927e+00, -7.2790e-01],\n",
       "          [-7.0619e-02, -9.9699e-01, -1.4520e-01,  ...,  1.1096e-02,\n",
       "            8.2167e-01, -8.1911e-01]],\n",
       "\n",
       "         [[ 4.9296e-01, -3.2073e-01,  6.9494e-01,  ...,  8.4651e-01,\n",
       "            2.9989e-01, -7.8703e-02],\n",
       "          [-2.8607e-01, -1.6307e+00,  2.3937e+00,  ..., -1.0739e+00,\n",
       "           -1.8014e-01,  1.8507e+00],\n",
       "          [ 3.3252e-01,  2.1991e-01,  9.9357e-01,  ...,  2.1244e-01,\n",
       "           -6.8687e-02,  2.4210e-01],\n",
       "          ...,\n",
       "          [ 2.7802e-01, -5.6167e-01, -1.4737e+00,  ..., -4.9149e-01,\n",
       "            1.1347e-02,  5.2398e-01],\n",
       "          [ 4.5153e-01, -2.3047e-01, -6.8024e-01,  ..., -9.0274e-01,\n",
       "            1.1562e-01,  1.6814e+00],\n",
       "          [ 4.2719e-01,  8.7935e-01, -6.2015e-01,  ...,  3.8661e-01,\n",
       "           -7.7240e-01, -9.6745e-01]],\n",
       "\n",
       "         [[-3.8230e-01, -3.3220e-01,  2.1901e-01,  ..., -6.1358e-01,\n",
       "           -1.6039e-01,  8.0628e-01],\n",
       "          [-4.0527e-01, -3.4760e-02, -5.1615e-01,  ...,  5.0913e-02,\n",
       "            1.3998e-01,  2.1964e-01],\n",
       "          [ 7.6372e-03, -2.3355e-01, -5.1971e-01,  ...,  4.8124e-01,\n",
       "           -6.1408e-01, -3.4856e-01],\n",
       "          ...,\n",
       "          [ 9.3494e-02, -3.9421e-01, -2.4997e-01,  ..., -5.0541e-01,\n",
       "           -7.3988e-01, -8.6302e-02],\n",
       "          [-5.3905e-02, -3.3753e-01, -1.7649e-01,  ...,  2.0339e-01,\n",
       "            9.0025e-02, -1.8219e-01],\n",
       "          [ 3.5060e-01, -2.6280e-01, -9.2581e-02,  ..., -1.0879e-01,\n",
       "           -3.0101e-01,  1.4032e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.0617e-01, -5.8183e-01, -8.5636e-02,  ..., -2.9942e-01,\n",
       "           -1.1016e-01, -1.5831e-01],\n",
       "          [-5.4817e-01,  1.5595e-01, -1.9360e-01,  ..., -3.3378e-02,\n",
       "            3.4735e-01, -3.8474e-02],\n",
       "          [-1.8753e-01, -5.3943e-01, -5.9133e-01,  ...,  5.6781e-01,\n",
       "           -3.8752e-01,  3.9604e-01],\n",
       "          ...,\n",
       "          [-6.0816e-02, -1.5780e-01, -2.7894e-01,  ..., -3.1417e-01,\n",
       "           -4.2506e-01, -4.4471e-01],\n",
       "          [-8.1769e-01,  6.5008e-02,  8.9964e-02,  ...,  1.5379e-01,\n",
       "            5.4957e-02, -9.9994e-01],\n",
       "          [-3.1829e-01, -6.1134e-02, -3.8422e-01,  ..., -1.1728e-01,\n",
       "           -4.2816e-02,  1.8966e-01]],\n",
       "\n",
       "         [[-7.2151e-01, -7.4328e-01, -3.1916e-01,  ..., -4.7115e-01,\n",
       "           -4.4844e-02, -9.5343e-02],\n",
       "          [-4.2600e-01, -1.6786e-01,  1.4356e-01,  ...,  5.3341e-02,\n",
       "           -1.5897e-01, -3.1730e-01],\n",
       "          [-3.5479e-01, -5.9595e-01,  2.8194e-01,  ..., -1.4970e-01,\n",
       "            6.5603e-02,  2.4807e-02],\n",
       "          ...,\n",
       "          [-2.6037e-01, -3.7980e-01, -1.6742e-01,  ..., -2.4548e-01,\n",
       "           -4.9891e-01,  5.4507e-02],\n",
       "          [-1.1087e-01,  7.9226e-02, -4.2522e-01,  ...,  8.6672e-02,\n",
       "           -5.6420e-01, -3.4713e-01],\n",
       "          [-2.0117e-01, -2.5057e-01, -7.4129e-01,  ..., -3.5626e-01,\n",
       "            1.7010e-01,  1.4360e-01]],\n",
       "\n",
       "         [[ 3.1072e-01,  9.9982e-01,  7.7832e-01,  ...,  9.5499e-01,\n",
       "            8.3869e-01,  1.0165e+00],\n",
       "          [ 4.1145e-01,  5.8112e-01,  7.9395e-01,  ...,  9.6819e-01,\n",
       "            1.0998e+00,  1.1913e+00],\n",
       "          [ 4.5759e-01,  6.9294e-01,  5.8283e-01,  ...,  8.8298e-01,\n",
       "            1.1804e+00,  8.3819e-01],\n",
       "          ...,\n",
       "          [ 5.1368e-01,  7.5257e-01,  5.8252e-01,  ...,  7.4090e-01,\n",
       "            7.0473e-01,  5.1672e-01],\n",
       "          [ 5.6713e-01,  7.1397e-01,  7.2491e-01,  ...,  9.3808e-01,\n",
       "            9.0700e-01,  8.3470e-01],\n",
       "          [ 3.2237e-01,  3.6429e-01,  4.1616e-01,  ...,  7.0161e-01,\n",
       "            1.7729e-01,  6.4358e-02]]],\n",
       "\n",
       "\n",
       "        [[[-5.4762e-01,  6.7629e-02, -2.0412e-01,  ..., -1.8149e-01,\n",
       "           -1.0404e+00, -7.8666e-01],\n",
       "          [-3.2998e-01, -4.4052e-01, -2.9025e-01,  ...,  1.2012e+00,\n",
       "           -1.1265e+00,  8.9467e-02],\n",
       "          [-4.5172e-01, -2.7524e-01, -5.1664e-01,  ...,  2.9767e-01,\n",
       "            6.2535e-01, -9.3890e-02],\n",
       "          ...,\n",
       "          [-2.1816e-01,  3.5468e-01, -2.5032e-01,  ..., -7.2777e-01,\n",
       "           -1.0305e+00,  6.0636e-01],\n",
       "          [ 1.7860e-01, -8.5468e-01, -3.4504e-01,  ..., -4.4876e-01,\n",
       "           -4.7820e-01, -2.0794e-01],\n",
       "          [-6.0341e-01, -2.3149e-01,  3.3431e-01,  ..., -4.1443e-01,\n",
       "           -6.3567e-01, -7.0110e-01]],\n",
       "\n",
       "         [[ 1.2028e+00, -1.2605e+00, -2.0800e-01,  ..., -1.2365e-01,\n",
       "            3.3132e-01,  9.1130e-01],\n",
       "          [-6.0996e-01, -8.8914e-01,  1.4918e+00,  ..., -2.1065e-01,\n",
       "            3.0753e-01,  3.7775e-01],\n",
       "          [ 5.3506e-01,  1.5775e-01,  1.9290e+00,  ..., -6.4618e-01,\n",
       "            4.1711e-02,  3.2528e-01],\n",
       "          ...,\n",
       "          [ 7.2662e-01,  1.1486e+00,  8.2499e-01,  ..., -1.2380e+00,\n",
       "            1.4070e+00,  6.7565e-01],\n",
       "          [-1.3492e+00,  1.2764e+00, -1.0105e-01,  ..., -9.1698e-01,\n",
       "           -3.3248e-01,  1.5093e+00],\n",
       "          [ 1.4486e-01,  4.0101e-01,  6.7289e-01,  ..., -4.0059e-01,\n",
       "           -1.5464e-02,  9.4832e-01]],\n",
       "\n",
       "         [[-7.5733e-01, -3.2200e-01, -5.2648e-01,  ...,  1.7754e-01,\n",
       "            3.6315e-01,  7.2620e-02],\n",
       "          [ 1.8756e-01,  1.7442e-01,  1.7502e-01,  ..., -7.6008e-01,\n",
       "           -6.7231e-01, -2.7219e-01],\n",
       "          [ 2.1446e-02,  4.9006e-01, -5.1634e-01,  ..., -5.0878e-01,\n",
       "           -1.0417e+00, -3.4424e-01],\n",
       "          ...,\n",
       "          [-7.4793e-01,  5.8754e-02, -4.5025e-01,  ...,  1.6706e-01,\n",
       "           -4.8772e-01,  3.9969e-01],\n",
       "          [ 3.3830e-01, -7.0156e-01,  6.6572e-01,  ..., -4.2474e-01,\n",
       "           -7.6536e-01, -6.3844e-01],\n",
       "          [ 1.5976e-03, -3.4784e-01,  4.8962e-02,  ..., -2.8824e-02,\n",
       "           -1.6209e-01,  8.8646e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-6.7518e-01, -4.3255e-01, -2.3258e-01,  ..., -3.5187e-01,\n",
       "           -1.7565e-01, -4.7869e-01],\n",
       "          [-4.4357e-01, -4.2636e-01, -7.4811e-02,  ..., -2.1030e-01,\n",
       "            9.8296e-02,  9.2754e-02],\n",
       "          [ 2.3768e-02, -1.9118e-01,  3.4331e-01,  ..., -6.5574e-01,\n",
       "           -6.4911e-01, -1.3272e-01],\n",
       "          ...,\n",
       "          [ 7.0034e-02, -1.0292e-01, -6.6522e-01,  ..., -3.1173e-01,\n",
       "           -2.3435e-01, -3.4677e-02],\n",
       "          [-1.2827e-01,  1.5387e-01,  4.2360e-01,  ..., -3.9442e-01,\n",
       "           -1.1839e-01, -3.2853e-01],\n",
       "          [-1.1035e-01, -1.7786e-01, -3.7222e-01,  ..., -2.2924e-01,\n",
       "           -2.1355e-01, -4.5233e-01]],\n",
       "\n",
       "         [[-5.8319e-01, -3.3395e-01, -3.1229e-01,  ...,  4.1136e-02,\n",
       "            1.1750e-02, -1.4114e-02],\n",
       "          [ 1.5790e-02, -5.6354e-01, -3.9854e-01,  ...,  3.4507e-01,\n",
       "            4.7041e-01,  1.6055e-01],\n",
       "          [-2.5879e-01,  1.2786e-02, -2.3457e-01,  ..., -3.5484e-01,\n",
       "           -3.3316e-02, -1.4082e-02],\n",
       "          ...,\n",
       "          [-3.6501e-01, -2.2168e-01, -3.4725e-01,  ..., -1.8727e-01,\n",
       "           -1.0913e-01, -6.0839e-01],\n",
       "          [ 2.1973e-01, -5.3663e-01, -3.2959e-01,  ...,  1.1239e-01,\n",
       "            1.4995e-02, -1.5431e-01],\n",
       "          [-1.2215e-01,  1.9086e-02, -2.3388e-01,  ..., -5.1892e-01,\n",
       "           -7.0363e-01, -2.6257e-01]],\n",
       "\n",
       "         [[ 4.3411e-01,  6.5503e-01,  5.3297e-01,  ...,  1.1304e+00,\n",
       "            1.0661e+00,  8.2664e-01],\n",
       "          [ 2.8907e-01,  6.9676e-01,  6.6373e-01,  ...,  7.5461e-01,\n",
       "            4.7337e-01,  6.4102e-01],\n",
       "          [ 2.4468e-01,  5.9070e-01,  7.0374e-01,  ...,  6.5790e-01,\n",
       "            3.9972e-01,  6.8211e-01],\n",
       "          ...,\n",
       "          [ 3.0288e-01,  6.7572e-01,  4.0124e-01,  ...,  5.9490e-01,\n",
       "            7.1528e-01,  7.3439e-01],\n",
       "          [ 7.1452e-01,  8.1484e-01,  7.2230e-01,  ...,  5.0403e-01,\n",
       "            3.8521e-01,  6.1924e-01],\n",
       "          [ 3.9729e-01,  2.8045e-01,  6.9961e-01,  ...,  3.7803e-01,\n",
       "            5.2377e-01,  5.0302e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.5415e-01, -6.6633e-01,  2.6385e-01,  ..., -2.0922e-01,\n",
       "            2.7100e-01, -1.1562e+00],\n",
       "          [-4.2982e-01,  2.9064e-01, -3.0873e-01,  ..., -4.6639e-01,\n",
       "            8.2668e-02,  4.5115e-01],\n",
       "          [-7.5383e-01, -5.6688e-01, -5.1884e-01,  ..., -4.7180e-01,\n",
       "           -2.2893e-01,  3.0863e-01],\n",
       "          ...,\n",
       "          [ 7.3516e-01, -9.1972e-01,  9.8301e-01,  ..., -3.6837e-01,\n",
       "            6.5800e-03, -4.6720e-01],\n",
       "          [-3.6482e-01, -1.0209e+00,  7.8467e-01,  ...,  6.7456e-02,\n",
       "           -1.6350e-01,  1.4215e-01],\n",
       "          [-1.0595e-01, -1.0901e+00, -4.0293e-01,  ...,  6.3705e-01,\n",
       "           -1.1235e+00, -9.5372e-02]],\n",
       "\n",
       "         [[-2.1654e-01,  8.0632e-01, -1.1166e-02,  ...,  1.0452e+00,\n",
       "            4.5211e-01,  7.7753e-01],\n",
       "          [-9.3437e-02,  6.3722e-02,  2.1414e-01,  ...,  7.1839e-01,\n",
       "            1.1535e+00, -1.4037e+00],\n",
       "          [ 2.5573e-01,  2.8126e-01,  7.1870e-01,  ..., -1.7283e+00,\n",
       "            1.2881e+00,  1.0543e+00],\n",
       "          ...,\n",
       "          [-4.6520e-01,  1.2334e+00, -9.9032e-01,  ..., -3.6956e-01,\n",
       "            1.9892e-01,  3.8914e-01],\n",
       "          [ 5.6226e-01,  1.3348e+00,  1.3600e+00,  ...,  5.5221e-02,\n",
       "            2.5922e-01,  1.2382e-01],\n",
       "          [-1.2687e+00, -7.2051e-02,  6.1793e-01,  ..., -7.1771e-02,\n",
       "            2.7795e-02,  3.3052e-01]],\n",
       "\n",
       "         [[ 2.9984e-03, -9.1547e-01, -3.6690e-02,  ..., -8.7763e-02,\n",
       "            5.0749e-01, -7.5078e-01],\n",
       "          [ 2.0080e-01, -3.8894e-01,  1.1675e-01,  ..., -5.6520e-01,\n",
       "           -2.8351e-01, -4.2491e-01],\n",
       "          [ 1.7034e-01,  3.8605e-01, -3.1173e-01,  ..., -1.7661e-01,\n",
       "           -1.1409e+00, -1.7523e-01],\n",
       "          ...,\n",
       "          [-1.5423e-02, -3.8008e-01,  3.0043e-01,  ...,  1.3820e-02,\n",
       "           -1.5643e-01, -2.0831e-01],\n",
       "          [-2.5172e-01, -6.2328e-01, -7.7319e-01,  ..., -4.3243e-01,\n",
       "            5.1941e-02, -7.3729e-01],\n",
       "          [ 3.2522e-01,  6.9927e-02, -3.3695e-01,  ...,  2.6162e-01,\n",
       "            3.2749e-01, -1.8884e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.1314e-01,  6.0922e-02, -5.4759e-01,  ...,  1.5990e-01,\n",
       "           -2.5903e-01, -5.9720e-01],\n",
       "          [-4.9198e-01, -2.6459e-01,  8.9231e-02,  ..., -2.1922e-01,\n",
       "           -1.5319e-01,  3.6419e-01],\n",
       "          [ 3.7395e-01, -4.4257e-01,  7.0693e-02,  ..., -2.3461e-01,\n",
       "           -5.7020e-02, -3.6637e-01],\n",
       "          ...,\n",
       "          [ 4.1229e-02,  4.9750e-01,  1.2508e-01,  ...,  9.6477e-02,\n",
       "           -3.9987e-01,  2.1591e-01],\n",
       "          [-5.8327e-01, -1.2358e-01, -2.3807e-01,  ..., -2.0027e-01,\n",
       "           -7.2255e-01,  1.6809e-01],\n",
       "          [ 1.3339e-01, -1.1742e-01,  3.0300e-01,  ..., -1.1309e-01,\n",
       "            7.0483e-02, -3.5298e-01]],\n",
       "\n",
       "         [[ 7.7569e-02, -4.0478e-01, -4.7608e-01,  ..., -2.8507e-01,\n",
       "           -4.6080e-01, -1.1926e+00],\n",
       "          [-3.0409e-01, -2.5064e-01, -4.1556e-01,  ..., -2.2753e-01,\n",
       "           -4.1090e-01,  1.6629e-03],\n",
       "          [-2.0936e-01,  9.7129e-02, -2.8825e-01,  ...,  1.8581e-01,\n",
       "           -3.4899e-01, -2.7069e-01],\n",
       "          ...,\n",
       "          [ 4.6548e-02,  4.6516e-01,  5.1788e-02,  ..., -1.0389e-01,\n",
       "           -1.5888e-01, -4.1103e-01],\n",
       "          [-6.3273e-01,  1.6058e-01,  7.3948e-02,  ..., -3.4026e-01,\n",
       "            8.2384e-02, -1.4578e-01],\n",
       "          [-1.8971e-01, -4.4588e-02, -4.9109e-01,  ..., -1.3649e-01,\n",
       "           -4.0348e-01, -6.6640e-02]],\n",
       "\n",
       "         [[ 5.6255e-01,  5.3917e-01,  9.7730e-01,  ...,  8.7030e-01,\n",
       "            8.6849e-01,  7.1530e-01],\n",
       "          [ 6.4871e-01,  6.3466e-01,  9.4155e-01,  ...,  1.1142e+00,\n",
       "            1.2390e+00,  2.2911e-01],\n",
       "          [ 4.7833e-01,  1.1407e+00,  6.5946e-01,  ...,  5.5420e-01,\n",
       "            9.0364e-01,  9.7315e-01],\n",
       "          ...,\n",
       "          [ 4.0643e-01,  5.4000e-01,  9.1466e-01,  ...,  5.1846e-01,\n",
       "            6.6777e-01,  6.9245e-01],\n",
       "          [ 7.3198e-01,  5.7114e-01,  7.0298e-01,  ...,  7.8632e-01,\n",
       "            6.7914e-01,  4.9217e-01],\n",
       "          [ 2.0964e-01,  6.3579e-01,  7.9936e-01,  ...,  7.3598e-01,\n",
       "            7.0669e-01,  3.1617e-01]]]], device='cuda:0',\n",
       "       grad_fn=<CheckpointFunctionBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb(in_rs1,emb)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model.input_blocks.1.0', qdiff.quant_block.QuantResBlock)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.full_name,type(rb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(rb,BaseQuantBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.input_blocks.1.0.in_layers.0 True\n",
      "model.input_blocks.1.0.in_layers.1 True\n",
      "model.input_blocks.1.0.emb_layers.0 True\n",
      "model.input_blocks.1.0.out_layers.0 True\n",
      "model.input_blocks.1.0.out_layers.1 True\n"
     ]
    }
   ],
   "source": [
    "for name, module in rb.named_modules():\n",
    "    if isinstance(module, (QuantModel,QuantOp)):\n",
    "        print( module.full_name,module.act_quantizer.leaf_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniformAffineQuantizer(bit=8, scale_method=max, symmetric=False, channel_wise=False, leaf_param=True)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.act_quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(rb.act_quantizer, 'delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.use_checkpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.eval()\n",
    "rb.requires_grad_(False)\n",
    "rb.use_checkpoint = False\n",
    "traced_resblock = torch.jit.trace(rb,(in_rs1,emb),_store_inputs=True)\n",
    "traced_resblock.save('traced_Qresblock_Qop.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ldm.modules.diffusionmodules.openaimodel.UNetModel"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.model.diffusion_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = model.model.diffusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 3, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unet.input_blocks),len(unet.middle_block),len(unet.output_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = lambda l : str(type(l)).split('.')[-1].split('\\'')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =[] \n",
    "a.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Conv2d'],\n",
       " ['ResBlock', 'SpatialTransformer'],\n",
       " ['ResBlock', 'SpatialTransformer'],\n",
       " ['Downsample'],\n",
       " ['ResBlock', 'SpatialTransformer'],\n",
       " ['ResBlock', 'SpatialTransformer'],\n",
       " ['Downsample'],\n",
       " ['ResBlock', 'SpatialTransformer'],\n",
       " ['ResBlock', 'SpatialTransformer'],\n",
       " ['Downsample'],\n",
       " ['ResBlock'],\n",
       " ['ResBlock']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_b =[]\n",
    "for i,inb_i in enumerate(unet.input_blocks):\n",
    "    in_b.append([])\n",
    "    for l in inb_i:\n",
    "        in_b[i].append(lt(l))\n",
    "in_b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ResBlock', 'SpatialTransformer', 'ResBlock']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_b=[]\n",
    "for l in unet.middle_block:\n",
    "    in_b.append(lt(l))\n",
    "in_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ResBlock'],\n",
       " ['ResBlock'],\n",
       " ['ResBlock', 'Upsample'],\n",
       " ['ResBlock', 'SpatialTransformer'],\n",
       " ['ResBlock', 'SpatialTransformer'],\n",
       " ['ResBlock', 'SpatialTransformer', 'Upsample'],\n",
       " ['ResBlock', 'SpatialTransformer'],\n",
       " ['ResBlock', 'SpatialTransformer'],\n",
       " ['ResBlock', 'SpatialTransformer', 'Upsample'],\n",
       " ['ResBlock', 'SpatialTransformer'],\n",
       " ['ResBlock', 'SpatialTransformer'],\n",
       " ['ResBlock', 'SpatialTransformer']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_b =[]\n",
    "for i,inb_i in enumerate(unet.output_blocks):\n",
    "    in_b.append([])\n",
    "    for l in inb_i:\n",
    "        in_b[i].append(lt(l))\n",
    "in_b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ldm.modules.diffusionmodules.openaimodel.ResBlock"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unet.output_blocks[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.output_blocks[5][0].updown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.nn.modules.conv.Conv2d,\n",
       " ldm.modules.diffusionmodules.openaimodel.ResBlock,\n",
       " ldm.modules.diffusionmodules.openaimodel.Downsample,\n",
       " ldm.modules.diffusionmodules.openaimodel.Upsample,\n",
       " ldm.modules.attention.SpatialTransformer)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2d = unet.input_blocks[0][0]\n",
    "rb = unet.input_blocks[1][0]\n",
    "us = unet.output_blocks[2][1]\n",
    "ds = unet.input_blocks[3][0]\n",
    "st = unet.input_blocks[1][1]\n",
    "type(c2d),type(rb),type(ds),type(us),type(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.use_conv,us.use_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "  (1): SiLU()\n",
       "  (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.in_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): SiLU()\n",
       "  (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.emb_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "  (1): SiLU()\n",
       "  (2): Dropout(p=0, inplace=False)\n",
       "  (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.out_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identity()"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.skip_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(GroupNorm(32, 320, eps=1e-06, affine=True),\n",
       " Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1)),\n",
       " Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1)))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.norm,st.proj_in,st.proj_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(st.transformer_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CrossAttention(\n",
       "   (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "   (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "   (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "   (qk_matmul): CrossQKMatMul()\n",
       "   (smv_matmul): CrossSMVMatMul()\n",
       "   (to_out): Sequential(\n",
       "     (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "     (1): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       " ),\n",
       " CrossAttention(\n",
       "   (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "   (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "   (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "   (qk_matmul): CrossQKMatMul()\n",
       "   (smv_matmul): CrossSMVMatMul()\n",
       "   (to_out): Sequential(\n",
       "     (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "     (1): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.transformer_blocks[0].attn1, st.transformer_blocks[0].attn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.conv.Conv2d'>\n"
     ]
    }
   ],
   "source": [
    "inb_i = unet.input_blocks[0]\n",
    "#len(inb_i)\n",
    "for l in inb_i:\n",
    "    print(type(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Conv2d'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNetModel(\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (input_blocks): ModuleList(\n",
       "    (0): TimestepEmbedSequential(\n",
       "      (0): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1-2): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (3): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (4): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (10-11): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (middle_block): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): SpatialTransformer(\n",
       "      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "      (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (attn1): CrossAttention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (qk_matmul): CrossQKMatMul()\n",
       "            (smv_matmul): CrossSMVMatMul()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (attn2): CrossAttention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (qk_matmul): CrossQKMatMul()\n",
       "            (smv_matmul): CrossSMVMatMul()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (output_blocks): ModuleList(\n",
       "    (0-1): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (2): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): Upsample(\n",
       "        (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (3-4): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Upsample(\n",
       "        (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Upsample(\n",
       "        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (10-11): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "    (1): SiLU()\n",
       "    (2): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.diffusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([3, 4, 64, 64]), torch.Size([3]), torch.Size([3, 77, 768])]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device='cpu'\n",
    "input_data = [torch.randn(input_shape).to(device) for input_shape in input_shapes]\n",
    "[input_data[0].shape,input_data[1].shape,input_data[2].shape]\n",
    "#input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tuple(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[2].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_unet = {'x':input_data[0],'timesteps':input_data[1],'context':input_data[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm.modules.diffusionmodules.util import timestep_embedding\n",
    "t_emb = timestep_embedding(input_data[1], model.model.diffusion_model.model_channels, repeat_only=False)\n",
    "emb = model.model.diffusion_model.time_embed(t_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1280])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_rs1 = c2d(input_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs1_out= rb._forward(in_rs1,emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rb1.svg'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make_dot(rs1_out, params=dict(list(rb.named_parameters()))).render(\"rb1\", format=\"svg\")\n",
    "make_dot(rs1_out, params=None).render(\"rb1\", format=\"svg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=rb.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't redefine method: forward on class: __torch__.ldm.modules.diffusionmodules.util.___torch_mangle_186.GroupNorm32 (of Python compilation unit at: 0x5594424b26a0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1223395/1136009176.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraced_resblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#._forward,example_inputs=[rb,in_rs1,emb])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#traced_resblock.save('script_resblock.pt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m         return _script_impl(\n\u001b[0m\u001b[1;32m   1433\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_script_impl\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_prepare_scriptable_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m         return torch.jit._recursive.create_script_module(\n\u001b[0m\u001b[1;32m   1147\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recursive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_methods_to_compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mAttributeTypeIsSupportedChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_script_module_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0;31m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m     \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \"\"\"\n\u001b[1;32m    648\u001b[0m             \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m             \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0;31m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;31m# always reuse the provided stubs_fn to infer the methods to compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 scripted = create_script_module_impl(\n\u001b[0m\u001b[1;32m    609\u001b[0m                     \u001b[0morig_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_concrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                 )\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0;31m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m     \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36m_construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \"\"\"\n\u001b[1;32m    648\u001b[0m             \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecursiveScriptModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m             \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0;31m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;31m# always reuse the provided stubs_fn to infer the methods to compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 scripted = create_script_module_impl(\n\u001b[0m\u001b[1;32m    609\u001b[0m                     \u001b[0morig_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_concrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                 )\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconcrete_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconcrete_type_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethods_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m         create_methods_and_properties_from_stubs(\n\u001b[0m\u001b[1;32m    637\u001b[0m             \u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_stubs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperty_stubs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0mproperty_rcbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolution_callback\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperty_stubs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m     concrete_type._create_methods_and_properties(\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0mproperty_defs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperty_rcbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_defs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_rcbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't redefine method: forward on class: __torch__.ldm.modules.diffusionmodules.util.___torch_mangle_186.GroupNorm32 (of Python compilation unit at: 0x5594424b26a0)"
     ]
    }
   ],
   "source": [
    "traced_resblock = torch.jit.script(rb)#._forward,example_inputs=[rb,in_rs1,emb])\n",
    "#traced_resblock.save('script_resblock.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_trace.py:168: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789116784/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  if a.grad is not None:\n"
     ]
    }
   ],
   "source": [
    "rb.eval()\n",
    "rb.requires_grad_(False)\n",
    "traced_resblock = torch.jit.trace_module(rb,{'_forward':(in_rs1,emb)},_store_inputs=True)\n",
    "traced_resblock.save('traced_resblock.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_1223395/2261766065.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m----> 2\u001b[0;31m     torch.onnx.export(rb._forward,(in_rs1,emb),'resblock.onnx',\n",
      "\u001b[0m\u001b[1;32m      3\u001b[0m                    input_names=['x','timestep_embedding'], output_names=['res_block_out'])\n",
      "\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining, dynamo)\u001b[0m\n",
      "\u001b[1;32m    549\u001b[0m         )\n",
      "\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 551\u001b[0;31m     _export(\n",
      "\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    553\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n",
      "\u001b[1;32m   1623\u001b[0m             )\n",
      "\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m-> 1625\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mexporter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1626\u001b[0m             val_keep_init_as_ip = _decide_keep_init_as_input(\n",
      "\u001b[1;32m   1627\u001b[0m                 \u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    111\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    112\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    115\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexporter_context\u001b[0;34m(model, mode, verbose)\u001b[0m\n",
      "\u001b[1;32m    178\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_beartype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeartype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    179\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexporter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_C_onnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainingMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 180\u001b[0;31m     with select_model_mode_for_export(\n",
      "\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    182\u001b[0m     \u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmode_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable_apex_o2_state_dict_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    111\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    112\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    115\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mdisable_apex_o2_state_dict_hook\u001b[0;34m(model)\u001b[0m\n",
      "\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScriptFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    140\u001b[0m         \u001b[0mmodel_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# type: ignore[var-annotated]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_dict_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"O2StateDictHook\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'modules'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch.onnx.export(rb._forward,(in_rs1,emb),'resblock.onnx',\n",
    "                   input_names=['x','timestep_embedding'], output_names=['res_block_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.diffusion_model.model_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.model.diffusion_model(*input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rnn_torchviz.png'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(yhat, params=dict(list(model.model.diffusion_model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNetModel(\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (input_blocks): ModuleList(\n",
       "    (0): TimestepEmbedSequential(\n",
       "      (0): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1-2): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (3): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (4): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): Downsample(\n",
       "        (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (10-11): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (middle_block): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): SpatialTransformer(\n",
       "      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "      (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (attn1): CrossAttention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (qk_matmul): CrossQKMatMul()\n",
       "            (smv_matmul): CrossSMVMatMul()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (attn2): CrossAttention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (qk_matmul): CrossQKMatMul()\n",
       "            (smv_matmul): CrossSMVMatMul()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (emb_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (output_blocks): ModuleList(\n",
       "    (0-1): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (2): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): Upsample(\n",
       "        (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (3-4): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Upsample(\n",
       "        (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Upsample(\n",
       "        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (10-11): 2 x TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "              (qk_matmul): CrossQKMatMul()\n",
       "              (smv_matmul): CrossSMVMatMul()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "    (1): SiLU()\n",
       "    (2): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.diffusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hiddenlayer as hl\n",
    "\n",
    "transforms = [ hl.transforms.Prune('Constant') ] #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Argument passed to at() was not in the map.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_788964/224666211.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusion_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheme\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTHEMES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rnn_hiddenlayer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/hiddenlayer/graph.py\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(model, args, input_names, transforms, framework_transforms)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpytorch_builder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFRAMEWORK_TRANSFORMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Argument args must be provided for Pytorch models.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mimport_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tensorflow\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtf_builder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFRAMEWORK_TRANSFORMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/hiddenlayer/pytorch_builder.py\u001b[0m in \u001b[0;36mimport_graph\u001b[0;34m(hl_graph, model, args, input_names, verbose)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Run the Pytorch graph to get a trace and generate a graph from it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trace_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mtorch_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorExportTypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mONNX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Dump list of nodes (DEBUG only)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    654\u001b[0m     \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_canonicalize_graph_fuser_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m     \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_peephole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m     \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_fuse_addmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Argument passed to at() was not in the map."
     ]
    }
   ],
   "source": [
    "graph = hl.build_graph(model.model.diffusion_model, input_data, transforms=transforms)\n",
    "graph.theme = hl.graph.THEMES['blue'].copy()\n",
    "graph.save('rnn_hiddenlayer', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 64, 64])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outout_unet= model.model.diffusion_model(**input_unet)\n",
    "outout_unet= model.model.diffusion_model(*input_data)\n",
    "\n",
    "outout_unet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "_Map_base::at",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_788964/1816734827.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'timesteps'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m torch.onnx.export(model.model.diffusion_model,(input_data[0],input_data[1],input_data[2]),\n\u001b[0m\u001b[1;32m      4\u001b[0m                    \u001b[0;34m'UNetModel.onnx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    input_names=input_names, output_names=output_names)\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining, dynamo)\u001b[0m\n\u001b[1;32m    549\u001b[0m         )\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     _export(\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1646\u001b[0m             \u001b[0m_validate_dynamic_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1648\u001b[0;31m             graph, params_dict, torch_out = _model_to_graph(\n\u001b[0m\u001b[1;32m   1649\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pre_trace_quant_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_jit_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1171\u001b[0m     \u001b[0mparams_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_named_param_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_trace_and_get_graph_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m     \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mprev_autocast_cache_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_autocast_cache_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_autocast_cache_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     trace_graph, torch_out, inputs_states = torch.jit._get_trace_graph(\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1495\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m     outs = ONNXTracedModule(\n\u001b[0m\u001b[1;32m   1498\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m     )(*args, **kwargs)\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         graph, out = torch._C._create_graph_by_tracing(\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0min_vars\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0minputs_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrace_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0minputs_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1544\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/qdiff/q-diffusion-org/ldm/modules/diffusionmodules/openaimodel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, timesteps, context, y, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;31m# import ipdb; ipdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0mhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiddle_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1544\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/qdiff/q-diffusion-org/ldm/modules/diffusionmodules/openaimodel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, emb, context, split)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimestepBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpatialTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1544\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/qdiff/q-diffusion-org/ldm/modules/diffusionmodules/openaimodel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, emb, split)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0man\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m \u001b[0mx\u001b[0m \u001b[0mC\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \"\"\"\n\u001b[0;32m--> 250\u001b[0;31m         return checkpoint(\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         )\n",
      "\u001b[0;32m/work/qdiff/q-diffusion-org/ldm/modules/diffusionmodules/util.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(func, inputs, params, flag)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: _Map_base::at"
     ]
    }
   ],
   "source": [
    "input_names = ['x','timesteps','context']\n",
    "output_names = ['h']\n",
    "torch.onnx.export(model.model.diffusion_model,(input_data[0],input_data[1],input_data[2]),\n",
    "                   'UNetModel.onnx',\n",
    "                   input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.onnx' has no attribute '_optimize_trace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_788964/1380675914.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.onnx' has no attribute '_optimize_trace'"
     ]
    }
   ],
   "source": [
    "torch.onnx._optimize_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "_Map_base::at",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_788964/2656900282.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraced_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusion_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#traced_model = torch.jit.trace(model.model.diffusion_model, input_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#traced_model = torch.jit.trace(model.model.diffusion_model, **input_unet)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m     \u001b[0mlog_torchscript_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m     traced_func = _trace_impl(\n\u001b[0m\u001b[1;32m   1001\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36m_trace_impl\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"example_kwarg_inputs should be a dict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m         return trace_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"forward\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m                 \u001b[0mexample_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m                 module._c._create_method_from_trace(\n\u001b[0m\u001b[1;32m   1276\u001b[0m                     \u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1544\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/qdiff/q-diffusion-org/ldm/modules/diffusionmodules/openaimodel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, timesteps, context, y, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;31m# import ipdb; ipdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0mhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiddle_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1544\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/qdiff/q-diffusion-org/ldm/modules/diffusionmodules/openaimodel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, emb, context, split)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimestepBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpatialTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1544\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/qdiff/q-diffusion-org/ldm/modules/diffusionmodules/openaimodel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, emb, split)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0man\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m \u001b[0mx\u001b[0m \u001b[0mC\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \"\"\"\n\u001b[0;32m--> 250\u001b[0;31m         return checkpoint(\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         )\n",
      "\u001b[0;32m/work/qdiff/q-diffusion-org/ldm/modules/diffusionmodules/util.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(func, inputs, params, flag)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: _Map_base::at"
     ]
    }
   ],
   "source": [
    "traced_model = torch.jit.trace(model.model.diffusion_model, (input_data[0],input_data[1],input_data[2]))\n",
    "#traced_model = torch.jit.trace(model.model.diffusion_model, input_data)\n",
    "\n",
    "#traced_model = torch.jit.trace(model.model.diffusion_model, **input_unet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "_Map_base::at",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_646602/930556156.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_torch_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusion_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/work/qdiff/mo_utils/mo_utils/utils/torch_utils.py\u001b[0m in \u001b[0;36mcreate_torch_script\u001b[0;34m(model, input_shapes, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtraced_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraced_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m     \u001b[0mlog_torchscript_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m     traced_func = _trace_impl(\n\u001b[0m\u001b[1;32m   1001\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36m_trace_impl\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"example_kwarg_inputs should be a dict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m         return trace_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"forward\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m                 \u001b[0mexample_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m                 module._c._create_method_from_trace(\n\u001b[0m\u001b[1;32m   1276\u001b[0m                     \u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1544\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/qdiff/q-diffusion-org/ldm/modules/diffusionmodules/openaimodel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, timesteps, context, y, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;31m# import ipdb; ipdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0mhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiddle_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1544\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/qdiff/q-diffusion-org/ldm/modules/diffusionmodules/openaimodel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, emb, context, split)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimestepBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpatialTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1544\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/qdiff/q-diffusion-org/ldm/modules/diffusionmodules/openaimodel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, emb, split)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0man\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m \u001b[0mx\u001b[0m \u001b[0mC\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \"\"\"\n\u001b[0;32m--> 250\u001b[0;31m         return checkpoint(\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         )\n",
      "\u001b[0;32m/work/qdiff/q-diffusion-org/ldm/modules/diffusionmodules/util.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(func, inputs, params, flag)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qdiff_reorg/lib/python3.8/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: _Map_base::at"
     ]
    }
   ],
   "source": [
    "create_torch_script(model.model.diffusion_model,input_shapes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qdiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
